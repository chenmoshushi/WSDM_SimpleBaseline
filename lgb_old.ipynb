{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### imports\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/numba/errors.py:137: UserWarning: Insufficiently recent colorama version found. Numba requires colorama >= 0.3.9\n",
      "  warnings.warn(msg)\n",
      "/home/iis519/.local/lib/python3.7/site-packages/sklearn/externals/joblib/__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2079282, 8)\n",
      "cpu: 48\n",
      "used cpu: 16\n",
      "77.18794895410538\n",
      "(2079282, 217)\n",
      "success\n",
      "87.85991539557774\n"
     ]
    }
   ],
   "source": [
    "%run preprocess.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "import multiprocessing as mp\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import time\n",
    "from tqdm.notebook import tqdm\n",
    "from util import pre_process\n",
    "import pickle\n",
    "import gc\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '../../data_2020/'\n",
    "model_path = 'models/'\n",
    "other_path = 'others/'\n",
    "n = 80\n",
    "drop_no_content = True\n",
    "drop_no_year = True\n",
    "drop_keywords = False\n",
    "drop_no_abstract = False\n",
    "MIN_LEN = 7\n",
    "workers = mp.cpu_count()//2\n",
    "paper_thd = 47500\n",
    "take = '_new'\n",
    "num_leaves = 64\n",
    "reg_alpha = 1\n",
    "reg_lambda = 0.1\n",
    "objective = 'binary'\n",
    "max_depth = -1\n",
    "learning_rate = 0.1\n",
    "min_child_samples = 5\n",
    "n_estimators = 5000\n",
    "subsample = 0.8\n",
    "colsample_bytree = 0.8\n",
    "seed = 9487"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load data\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2079282, 5)\n",
      "2079282\n",
      "2079282\n",
      "(2079282, 217)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((2079282, 217), (2079282, 5))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for prediction\n",
    "test = pd.read_csv(data_path+'validation.csv')\n",
    "\n",
    "# get meta\n",
    "test_data = pd.read_csv(data_path+'test_data_merge_{}_{}.csv'.format(n, paper_thd))[['paper_id','description_id','journal','year','keywords']].fillna('')\n",
    "abstract = pd.read_csv(data_path+'test_data_merge_{}_{}.csv'.format(n, paper_thd))[['abstract']].fillna('NO_CONTENT')\n",
    "print(test_data.shape)\n",
    "\n",
    "# drop\n",
    "keep_idxs = []\n",
    "if drop_no_content:\n",
    "    test_have_content_idxs = test_data.index[test_data['journal'] != 'no-content'].tolist()\n",
    "    keep_idxs += test_have_content_idxs\n",
    "    print(len(keep_idxs))\n",
    "if drop_no_year:\n",
    "    test_have_year_idxs = test_data.index[test_data['year'] != ''].tolist()\n",
    "    if len(keep_idxs) > 0:\n",
    "        keep_idxs = list(set(test_have_year_idxs).intersection(set(keep_idxs)))\n",
    "    else:\n",
    "        keep_idxs += test_have_year_idxs\n",
    "    print(len(keep_idxs))\n",
    "if drop_keywords:\n",
    "    test_no_keywords_idxs = test_data.index[test_data['keywords'] == ''].tolist()\n",
    "    if len(keep_idxs) > 0:\n",
    "        keep_idxs = list(set(test_no_keywords_idxs).intersection(set(keep_idxs)))\n",
    "    else:\n",
    "        keep_idxs += test_have_year_idxs\n",
    "    print(len(keep_idxs))\n",
    "if drop_no_abstract:\n",
    "    test_have_abstract_idxs = abstract.index[abstract['abstract'] != 'NO_CONTENT'].tolist()\n",
    "    if len(keep_idxs) > 0:\n",
    "        keep_idxs = list(set(test_have_abstract_idxs).intersection(set(keep_idxs)))\n",
    "    else:\n",
    "        keep_idxs += test_have_abstract_idxs\n",
    "    print(len(keep_idxs))\n",
    "        \n",
    "# sort indexs\n",
    "keep_idxs = sorted(keep_idxs)\n",
    "if not drop_no_content and not drop_no_year and not drop_keywords and not drop_no_abstract:\n",
    "    keep_idxs = [i for i in range(test_data.shape[0])]\n",
    "    \n",
    "# get feature\n",
    "test_x = pd.read_csv(data_path+'test_data_merge_{}_{}_featall.csv'.format(n, paper_thd))\n",
    "print(test_x.shape)\n",
    "test_x = test_x.iloc[keep_idxs]\n",
    "test_data = test_data.iloc[keep_idxs]\n",
    "\n",
    "test_x.shape, test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # drop feature\n",
    "# drop = 'psu_key'\n",
    "# cols = [col for col in test_x.columns.values if not col.__contains__(drop)]\n",
    "# train_x = train_x[cols]\n",
    "# test_x = test_x[cols]\n",
    "# train_x.shape, test_x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save & load model\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save\n",
    "# for i in range(len(models)):\n",
    "#     with open(model_path+'lgb_{}'.format(i), 'wb') as f:\n",
    "#         pickle.dump(models[i], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load\n",
    "models = []\n",
    "for i in range(10):\n",
    "    with open(model_path+'lgb{}_{}'.format(take, i), 'rb') as f:\n",
    "        models.append(pickle.load(f))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### prediction\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # average\n",
    "# test_pred = np.zeros(test_x.shape[0])\n",
    "# for pred in test_preds:\n",
    "#     test_pred += pred\n",
    "# test_data['pred'] = test_pred/len(models)\n",
    "# temp = test_data[['description_id','pred']].groupby('description_id')['pred'].agg(lambda x: list(x))\n",
    "# test_data['preb_list'] = test_data['description_id'].map(temp)\n",
    "# temp = test_data[['description_id','paper_id']].groupby('description_id')['paper_id'].agg(lambda x: list(x))\n",
    "# test_data['paper_id_list'] = test_data['description_id'].map(temp)\n",
    "# test_sub = test_data.drop_duplicates('description_id')\n",
    "\n",
    "# def get_topn_paper_id(paper_id_list,preb_list,tag):\n",
    "#     index = np.argsort(preb_list)\n",
    "#     if len(index) < tag:\n",
    "#         tag = len(index)\n",
    "#         return np.nan\n",
    "#     return paper_id_list[index[-tag]]\n",
    "\n",
    "# test_sub['p1'] = list(map(lambda x,y:get_topn_paper_id(x,y,1),test_sub['paper_id_list'],test_sub['preb_list']))\n",
    "# test_sub['p2'] = list(map(lambda x,y:get_topn_paper_id(x,y,2),test_sub['paper_id_list'],test_sub['preb_list']))\n",
    "# test_sub['p3'] = list(map(lambda x,y:get_topn_paper_id(x,y,3),test_sub['paper_id_list'],test_sub['preb_list']))\n",
    "# test_sub = test_sub[['description_id','p1','p2','p3']]\n",
    "\n",
    "# sub = test[['description_id']]\n",
    "# sub = sub.fillna('none')\n",
    "# sub = sub.merge(test_sub, on='description_id', how='left')\n",
    "# sub.loc[sub['description_id']=='none', 'description_id'] = ''\n",
    "# sub.to_csv('predictions/prediction_{}.csv'.format(seed), index=False, sep=',', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7bdf984e7784bb3ab96dbb083e78e41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71dcfa41bf434e0498b6df532e418902",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CPU times: user 1h 3min 1s, sys: 50.4 s, total: 1h 3min 51s\n",
      "Wall time: 4min 20s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# vote\n",
    "def get_topn_paper_id(paper_id_list,preb_list,tag):\n",
    "    index = np.argsort(preb_list)\n",
    "    if len(index) < tag:\n",
    "        tag = len(index)\n",
    "        return np.nan\n",
    "    return paper_id_list[index[-tag]]\n",
    "\n",
    "did2vote = {did[0]: {} for did in test[['description_id']].fillna('none').values}\n",
    "test_preds = [model.predict_proba(test_x)[:, 1] for model in tqdm(models)]\n",
    "\n",
    "for test_pred in tqdm(test_preds):\n",
    "    test_data['pred'] = test_pred\n",
    "    temp = test_data[['description_id','pred']].groupby('description_id')['pred'].agg(lambda x: list(x))\n",
    "    test_data['preb_list'] = test_data['description_id'].map(temp)\n",
    "    temp = test_data[['description_id','paper_id']].groupby('description_id')['paper_id'].agg(lambda x: list(x))\n",
    "    test_data['paper_id_list'] = test_data['description_id'].map(temp)\n",
    "    test_sub = test_data.drop_duplicates('description_id')\n",
    "\n",
    "    test_sub['p1'] = list(map(lambda x,y:get_topn_paper_id(x,y,1),test_sub['paper_id_list'],test_sub['preb_list']))\n",
    "    test_sub['p2'] = list(map(lambda x,y:get_topn_paper_id(x,y,2),test_sub['paper_id_list'],test_sub['preb_list']))\n",
    "    test_sub['p3'] = list(map(lambda x,y:get_topn_paper_id(x,y,3),test_sub['paper_id_list'],test_sub['preb_list']))\n",
    "    test_sub['p4'] = list(map(lambda x,y:get_topn_paper_id(x,y,4),test_sub['paper_id_list'],test_sub['preb_list']))\n",
    "    test_sub['p5'] = list(map(lambda x,y:get_topn_paper_id(x,y,5),test_sub['paper_id_list'],test_sub['preb_list']))\n",
    "    test_sub['p6'] = list(map(lambda x,y:get_topn_paper_id(x,y,6),test_sub['paper_id_list'],test_sub['preb_list']))\n",
    "    test_sub['p7'] = list(map(lambda x,y:get_topn_paper_id(x,y,7),test_sub['paper_id_list'],test_sub['preb_list']))\n",
    "    test_sub['p8'] = list(map(lambda x,y:get_topn_paper_id(x,y,8),test_sub['paper_id_list'],test_sub['preb_list']))\n",
    "    test_sub = test_sub[['description_id','p1','p2','p3','p4','p5','p6','p7','p8']]\n",
    "    \n",
    "    # vote\n",
    "    weights = {1: 10, 2: 8, 3: 6, 4: 5, 5: 4, 6: 3, 7: 2, 8: 1}\n",
    "    for row in test_sub.values:\n",
    "        for i in range(1, 9):\n",
    "            if row[i] not in did2vote[row[0]]:\n",
    "                did2vote[row[0]][row[i]] = weights[i]\n",
    "            else:\n",
    "                did2vote[row[0]][row[i]] += weights[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99d6badb5fa14da785cbceb8100eb0e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=34428.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_sub = []\n",
    "for did in tqdm(list(did2vote.keys())):\n",
    "    tmp = [(key, did2vote[did][key]) for key in list(did2vote[did].keys())]\n",
    "    tmp.sort(key=lambda x: x[1], reverse=True)\n",
    "    top3 = []\n",
    "    for pid in tmp:\n",
    "        if type(pid[0]) != float:\n",
    "            top3.append(pid[0])\n",
    "            if len(top3) >= 3:\n",
    "                break\n",
    "    test_sub.append([did]+top3)\n",
    "test_sub = pd.DataFrame(test_sub, columns=['description_id','p1','p2','p3'])\n",
    "\n",
    "sub = test[['description_id']]\n",
    "sub = sub.merge(test_sub, on='description_id', how='left')\n",
    "sub.to_csv('predictions/prediction_{}_{}_{}.csv'.format(seed, paper_thd, n), index=False, sep=',', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.006690240213476242"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(sum(test_preds)/len(test_preds[0]))/len(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description_id</th>\n",
       "      <th>p1</th>\n",
       "      <th>p2</th>\n",
       "      <th>p3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [description_id, p1, p2, p3]\n",
       "Index: []"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub[pd.isnull(sub).any(axis=1)]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
