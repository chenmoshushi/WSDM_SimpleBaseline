{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### prediction generating procedure\n",
    "***\n",
    "+ run `preprocess` cell (done)\n",
    "+ run the following steps at the same time for testing data\n",
    "    + recall\n",
    "        + five kinds of recall\n",
    "            + bm25: run `recall - BM25` cell (done)\n",
    "            + idf: run `recall - idf` cell (done)\n",
    "            + s2v: run `sent2vec_recall.ipynb` directly (done)\n",
    "            + blue: run `sent_bert_blue.ipynb` in hpcuda then run `bluebert_recall.ipynb` directly (done)\n",
    "            + key: run `keywords_recall.ipynb` directly (done)\n",
    "        + run the next three cells to merge all of them\n",
    "    + embeddings for feature\n",
    "        + `gen_vectors.ipynb`: get Word2Vec, FastText, and SIF (done)\n",
    "        + `bert.ipynb`: get `description2embedding_pre.pkl` (done)\n",
    "        + `sent2vec_embedding.ipynb`: get `description2embedding_s2v.pkl` (done)\n",
    "        + `sent_bert_sci.ipynb`: get `description2embedding.pkl` (done)\n",
    "+ run super fat `get features` cell after the above are all done\n",
    "+ run `lgb.ipynb` directly to get prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/numba/errors.py:137: UserWarning: Insufficiently recent colorama version found. Numba requires colorama >= 0.3.9\n",
      "  warnings.warn(msg)\n",
      "/home/iis519/.local/lib/python3.7/site-packages/sklearn/externals/joblib/__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from gensim.summarization.bm25 import BM25\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.callbacks import CallbackAny2Vec\n",
    "from gensim import corpora, similarities, models\n",
    "from gensim.summarization import bm25\n",
    "import fasttext\n",
    "from fse.models.base_s2v import BaseSentence2VecModel\n",
    "\n",
    "import Levenshtein\n",
    "import pandas as pd\n",
    "import swifter\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import pickle\n",
    "import os\n",
    "import re\n",
    "import csv\n",
    "import multiprocessing as mp\n",
    "from multiprocessing import Process, cpu_count, Manager, Pool\n",
    "from sklearn.externals import joblib\n",
    "import time\n",
    "import h5py\n",
    "import gc\n",
    "import collections\n",
    "import warnings\n",
    "from util import *\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### parameter\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '../../data_2020/'\n",
    "model_path = 'models/'\n",
    "other_path = 'others/'\n",
    "ns = {'bm25': 40, 'idf': 10, 's2v': 20, 'blue': 5, 'key': 5}\n",
    "thds = {'bm25': 400, 'idf': 400, 's2v': 400, 'blue': 400, 'key': 400}\n",
    "n = sum(list(ns.values()))\n",
    "drop_no_content = True\n",
    "test_only = True\n",
    "train_only = False\n",
    "MIN_LEN = 7\n",
    "idf_thd = 5.0\n",
    "paper_thd = 47500\n",
    "workers = mp.cpu_count()//3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf\n",
    "def one(x):\n",
    "    return (x+1e-10)/(x+1e-10)\n",
    "\n",
    "# idf\n",
    "def df2idf_thd(docfreq, totaldocs, log_base=2.0, add=0.0):\n",
    "    idf = add + np.log(float(totaldocs) / docfreq) / np.log(log_base)\n",
    "    return idf if idf >= idf_thd else add"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### preprocess\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(data_path+'train_release.csv')\n",
    "test = pd.read_csv(data_path+'validation.csv')\n",
    "candidate = pd.read_csv(data_path+'candidate_paper.csv')\n",
    "train = train[~train['description_id'].isnull()]\n",
    "candidate = candidate[~candidate['paper_id'].isnull()]\n",
    "\n",
    "print(train.isna().sum())\n",
    "print(test.isna().sum())\n",
    "print(candidate.shape)\n",
    "\n",
    "def digest(text):\n",
    "    backup = text[:]\n",
    "    text = text.replace('al.', '').split('. ')\n",
    "    t=''\n",
    "    pre_text=[]\n",
    "    len_text=len(text)-1\n",
    "    add=True\n",
    "    pre=''\n",
    "    while len_text>=0:\n",
    "        index=text[len_text]\n",
    "        index+=pre\n",
    "        if len(index.split(' '))<=3 :\n",
    "            add=False\n",
    "            pre=index+pre\n",
    "        else:\n",
    "            add=True\n",
    "            pre=''\n",
    "        if add:\n",
    "            pre_text.append(index)\n",
    "        len_text-=1\n",
    "    if len(pre_text)==0:\n",
    "        pre_text=text\n",
    "    pre_text.reverse()\n",
    "    for index in pre_text:\n",
    "        if index.find('[**##**]') != -1:\n",
    "            index = re.sub(r'[\\[|,]+\\*\\*\\#\\#\\*\\*[\\]|,]+','',index)\n",
    "            index+='. '\n",
    "            t+=index\n",
    "    return t\n",
    "\n",
    "train['key_text'] = train['description_text'].swifter.allow_dask_on_strings().apply(lambda x:digest(x) if str(x)!='nan' else '')\n",
    "test['key_text'] = test['description_text'].swifter.allow_dask_on_strings().apply(lambda x:digest(x) if str(x)!='nan' else '')\n",
    "\n",
    "train['key_text_pre'] = train['key_text'].swifter.allow_dask_on_strings().apply(lambda x:' '.join(pre_process(x) if str(x)!='nan' else ''))\n",
    "test['key_text_pre'] = test['key_text'].swifter.allow_dask_on_strings().apply(lambda x:' '.join(pre_process(x) if str(x)!='nan' else ''))\n",
    "\n",
    "train['description_text_pre'] = train['description_text'].swifter.allow_dask_on_strings().apply(lambda x:' '.join(pre_process(x) if str(x)!='nan' else ''))\n",
    "test['description_text_pre'] = test['description_text'].swifter.allow_dask_on_strings().apply(lambda x:' '.join(pre_process(x) if str(x)!='nan' else ''))\n",
    "\n",
    "train.to_csv(data_path+'train_pre.csv',index=False)\n",
    "test.to_csv(data_path+'test_pre.csv',index=False)\n",
    "\n",
    "candidate['title_pro'] = candidate['title'].swifter.allow_dask_on_strings().apply(lambda x:' '.join(pre_process(x) if str(x)!='nan' else ''))\n",
    "candidate['abstract_pre'] = candidate['abstract'].swifter.allow_dask_on_strings().apply(lambda x:' '.join(pre_process(x) if str(x)!='nan' and x!='NO_CONTENT' else ''))\n",
    "candidate['keywords'] = candidate['keywords'].swifter.allow_dask_on_strings().apply(lambda x: x.lower() if str(x)!='nan' else '')\n",
    "candidate.to_csv(data_path+'candidate_paper_pre.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### prepare recall\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "papers = pd.read_csv(data_path+'candidate_paper.csv')\n",
    "papers = papers[papers['paper_id'].notnull()]\n",
    "print(papers.shape)\n",
    "\n",
    "# fillna\n",
    "papers['abstract'] = papers['abstract'].fillna('')\n",
    "papers['title'] = papers['title'].fillna('')\n",
    "papers['keywords'] = papers['keywords'].fillna('')\n",
    "\n",
    "# concate all\n",
    "train = papers['title'].values + ' ' + \\\n",
    "        papers['abstract'].values + ' ' + \\\n",
    "        papers['keywords'].apply(lambda x: x.replace(';', ' ')).values\n",
    "train_item_id = list(papers['paper_id'].values)\n",
    "\n",
    "# save paper_id\n",
    "with open(other_path+'paper_id.pkl', 'wb') as fw:\n",
    "    pickle.dump(train_item_id, fw)\n",
    "\n",
    "# save paper content\n",
    "with open(other_path+'train_content.pkl', 'wb') as fw:\n",
    "    with Pool(processes=workers) as pool:\n",
    "        train = pool.map(pre_process, tqdm(train))\n",
    "    pickle.dump(train, fw)\n",
    "\n",
    "dictionary = corpora.Dictionary(train)\n",
    "corpus = [dictionary.doc2bow(text) for text in train]\n",
    "\n",
    "# tfidf\n",
    "tfidf_model = models.TfidfModel(corpus, \n",
    "                                wlocal=one,\n",
    "                                wglobal=df2idf_thd,\n",
    "                                dictionary=dictionary)\n",
    "corpus_tfidf = tfidf_model[corpus]\n",
    "\n",
    "# save things of tfidf\n",
    "dictionary.save(other_path+'train_dictionary.dict')\n",
    "tfidf_model.save(other_path+'train_idf.model')\n",
    "corpora.MmCorpus.serialize(other_path+'train_corpuse.mm', corpus)\n",
    "featurenum = len(dictionary.token2id.keys())\n",
    "index = similarities.SparseMatrixSimilarity(corpus_tfidf, num_features=featurenum)\n",
    "index.save(other_path+'train_index.index')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### recall - BM25\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# run the whole cell except the train part\n",
    "dictionary = corpora.Dictionary.load(other_path+'train_dictionary.dict')\n",
    "index = similarities.SparseMatrixSimilarity.load(other_path+'train_index.index')\n",
    "item_id_list = joblib.load(other_path+'paper_id.pkl')\n",
    "with open(other_path+'train_content.pkl', 'rb') as fr:\n",
    "    corpus = pickle.load(fr)\n",
    "\n",
    "# bm25\n",
    "print('get bm25')\n",
    "bm25Model = BM25(corpus)\n",
    "\n",
    "def get_recall_number_bm25(val, n):\n",
    "    docs = val['description_text'].values\n",
    "    ids = val['description_id'].values\n",
    "    submit = np.zeros((len(docs), n+1)).astype(np.str)\n",
    "    count = len(docs)\n",
    "    bar = tqdm(range(count))\n",
    "    for i in bar:\n",
    "        doc = docs[i]\n",
    "        id = ids[i]\n",
    "        scores = np.array(bm25Model.get_scores(doc))\n",
    "        related_doc_indices = scores.argsort()[:-n-1:-1]\n",
    "        col = [id]+[item_id_list[index] for index in related_doc_indices]\n",
    "        submit[i] = col\n",
    "    return submit\n",
    "\n",
    "def pool_extract_bm25_tfidf(data, f, n, chunk_size, worker=5):\n",
    "    cpu_worker = os.cpu_count()\n",
    "    print('cpu: {}'.format(cpu_worker))\n",
    "    if worker == -1 or worker > cpu_worker:\n",
    "        worker = cpu_worker\n",
    "    print('used cpu: {}'.format(worker))\n",
    "    t1 = time.time()\n",
    "    len_data = len(data)\n",
    "    start = 0\n",
    "    end = 0\n",
    "    p = Pool(worker)\n",
    "    res = []\n",
    "    pbar = tqdm(total=worker)\n",
    "    def update(*a):\n",
    "        pbar.update()\n",
    "    while end < len_data:\n",
    "        end = start + chunk_size\n",
    "        if end > len_data:\n",
    "            end = len_data\n",
    "        rslt = p.apply_async(f, (data[start:end],n), callback=update)\n",
    "        start = end\n",
    "        res.append(rslt)\n",
    "    p.close()\n",
    "    p.join()\n",
    "    t2 = time.time()\n",
    "    print((t2 - t1)/60)\n",
    "    results = np.concatenate([i.get() for i in res], axis=0)\n",
    "    return results\n",
    "\n",
    "# valid\n",
    "valid = pd.read_csv(data_path+'test_pre.csv')\n",
    "valid['key_text_pre'] = valid['key_text_pre'].apply(lambda x: x.split(' ') if str(x)!='nan' else [])\n",
    "valid['key_text_pre_len'] = valid['key_text_pre'].apply(lambda x: len(x))\n",
    "valid.loc[valid['key_text_pre_len'] < MIN_LEN, 'key_text_pre'] = valid.loc[valid['key_text_pre_len'] < MIN_LEN][\n",
    "    'description_text'].apply(lambda x: pre_process(re.sub(r'[\\[|,]+\\*\\*\\#\\#\\*\\*[\\]|,]+','',x))).values\n",
    "\n",
    "ids = list(valid['description_id'].values)\n",
    "docs = list(valid['key_text_pre'].values)\n",
    "print(valid.shape)\n",
    "\n",
    "valid = pd.DataFrame({'description_id': ids, 'description_text': docs})\n",
    "submit = pool_extract_bm25_tfidf(valid, get_recall_number_bm25, n, valid.shape[0]//workers+1, worker=workers)\n",
    "df = pd.DataFrame(submit)\n",
    "df.to_csv(data_path+'test_pairs_{}number_bm25_2.csv'.format(n), header=None, index=False)\n",
    "\n",
    "# train\n",
    "train = pd.read_csv(data_path+'train_pre.csv')\n",
    "train['key_text_pre'] = train['key_text_pre'].apply(lambda x: x.split(' ') if str(x)!='nan' else [])\n",
    "train['key_text_pre_len'] = train['key_text_pre'].apply(lambda x: len(x))\n",
    "train.loc[train['key_text_pre_len'] < MIN_LEN, 'key_text_pre'] = train.loc[train['key_text_pre_len'] < MIN_LEN][\n",
    "    'description_text'].apply(lambda x: pre_process(re.sub(r'[\\[|,]+\\*\\*\\#\\#\\*\\*[\\]|,]+', '', x))).values\n",
    "\n",
    "ids = list(train['description_id'].values)\n",
    "docs = list(train['key_text_pre'].values)\n",
    "print(train.shape)\n",
    "\n",
    "train = pd.DataFrame({'description_id': ids, 'description_text': docs})\n",
    "submit = pool_extract_bm25_tfidf(train, get_recall_number_bm25, n, train.shape[0]//workers+1, worker=workers)\n",
    "df = pd.DataFrame(submit)\n",
    "df.to_csv(data_path+'train_pairs_{}number_bm25_2.csv'.format(n), header=None, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### recall - idf\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# run the whole cell except the train part\n",
    "tfidf = models.TfidfModel.load(other_path+\"train_idf.model\")\n",
    "train_index = index\n",
    "\n",
    "def get_recall_number_tfidf(val, n):\n",
    "    docs = val['description_text'].values\n",
    "    ids = val['description_id'].values\n",
    "    submit = np.zeros((len(docs), n+1)).astype(np.str)\n",
    "    count = len(docs)\n",
    "    bar = tqdm(range(count))\n",
    "    for i in bar:\n",
    "        doc = docs[i]\n",
    "        id = ids[i]\n",
    "        vec = dictionary.doc2bow(doc)\n",
    "        test_vec = tfidf[vec]\n",
    "        sim = train_index.get_similarities(test_vec)\n",
    "        related_doc_indices = sim.argsort()[:-n-1:-1]\n",
    "        col = [id]+[item_id_list[index] for index in related_doc_indices]\n",
    "        submit[i] = col\n",
    "    return submit\n",
    "\n",
    "# valid\n",
    "submit = pool_extract_bm25_tfidf(valid, get_recall_number_tfidf, n, valid.shape[0]//workers+1, worker=workers)\n",
    "df = pd.DataFrame(submit)\n",
    "df.to_csv(data_path+'test_pairs_{}number_idf_2.csv'.format(n), header=None, index=False)\n",
    "\n",
    "# train\n",
    "submit = pool_extract_bm25_tfidf(train, get_recall_number_tfidf, n, train.shape[0]//workers+1, worker=workers)\n",
    "df = pd.DataFrame(submit)\n",
    "df.to_csv(data_path+'train_pairs_{}number_idf_2.csv'.format(n), header=None, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### combine above\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recall bm25...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e26be6a01754b93a12a23a3dfb9623e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=34428.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0.2679214592773324\n",
      "recall idf...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04b4a0ed2f8e4f13b5990195d4fc7020",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=34428.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0.046589984896014874\n",
      "recall s2v...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c469adec1d548c98fabc6bff6af40ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=34428.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0.10889392355059835\n",
      "recall blue...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cf00f5ee44e441594bb7526a51ca3a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=34428.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0.00787149994190775\n",
      "recall key...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77ce90f2d30e474e9c19eb350836a05c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=34428.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0.7238294411525502\n",
      "(2476883, 6)\n"
     ]
    }
   ],
   "source": [
    "# # change `test_only` to be True and everything will work fine\n",
    "train_pre = pd.read_csv(data_path+'train_pre.csv')\n",
    "test_pre = pd.read_csv(data_path+'test_pre.csv')\n",
    "candidate_pre = pd.read_csv(data_path+'candidate_paper_pre.csv')\n",
    "\n",
    "pid2journal = candidate_pre[['paper_id', 'journal']]\n",
    "pid2journal = {row[0]: row[1] for row in pid2journal.values}\n",
    "pid2year = candidate_pre[['paper_id', 'year']].fillna(-1)\n",
    "pid2year = {row[0]: row[1] for row in pid2year.values}\n",
    "pid2abstract = candidate_pre[['paper_id', 'abstract_pre']].fillna('')\n",
    "pid2abstract = {row[0]: row[1] for row in pid2abstract.values}\n",
    "pid2intrain = {pid: 1 for pid in train_pre['paper_id'].values}\n",
    "pid2idx = {pid: idx for idx, pid in enumerate(candidate_pre['paper_id'])}\n",
    "\n",
    "train_pre['label'] = 1\n",
    "train_pre = train_pre.drop_duplicates('description_id')\n",
    "\n",
    "def get_recall(path, take, filter_content=False):\n",
    "    total = 500 if take == 'key' else 1000\n",
    "    data = pd.read_csv(path, header=None, names=['description_id']+[i for i in range(1, total+1)]).drop_duplicates('description_id').values\n",
    "    description_id = []\n",
    "    pid = []\n",
    "    global count\n",
    "    for i in tqdm(data, total=data.shape[0]):\n",
    "        if not filter_content:\n",
    "            description_id.extend([i[0]]*ns[take])\n",
    "            pid.extend(list(i[1:ns[take]+1]))\n",
    "        else:\n",
    "            tmp = []\n",
    "            for id in list(i[1: thds[take]+1]):\n",
    "                if ns[take] == 0:\n",
    "                    break\n",
    "                if (not pd.isna(id)) and pid2journal[id] != 'no-content' \\\n",
    "                                     and pid2year[id] != -1 \\\n",
    "                                     and pid2idx[id] < paper_thd:\n",
    "                    tmp.append(id)\n",
    "                    if len(tmp) >= ns[take]:\n",
    "                        break\n",
    "            if len(tmp) < ns[take]:\n",
    "                count += 1\n",
    "            description_id.extend([i[0]]*len(tmp))\n",
    "            pid.extend(tmp)\n",
    "    return pd.DataFrame({'description_id':description_id, 'pid':pid})\n",
    "\n",
    "# bm25\n",
    "print('recall bm25...')\n",
    "count = 0\n",
    "if not train_only:\n",
    "    re_path = data_path+'test_pairs_1000number_bm25.csv'\n",
    "    test_recall = get_recall(re_path, 'bm25', drop_no_content)\n",
    "    test_data_bm25 = test_pre.merge(test_recall, on='description_id', how='inner')\n",
    "    print(count/test_pre.shape[0])\n",
    "\n",
    "count = 0\n",
    "if not test_only:\n",
    "    re_path = data_path+'train_pairs_1000number_bm25.csv'\n",
    "    train_recall = get_recall(re_path, 'bm25', False)\n",
    "    train_data_bm25 = train_pre.merge(train_recall, on='description_id', how='inner')\n",
    "    print(count/train_pre.shape[0])\n",
    "    \n",
    "# idf\n",
    "print('recall idf...')\n",
    "count = 0\n",
    "if not train_only:\n",
    "    re_path = data_path+'test_pairs_1000number_idf.csv'\n",
    "    test_recall = get_recall(re_path, 'idf', drop_no_content)\n",
    "    test_data_idf = test_pre.merge(test_recall, on='description_id', how='inner')\n",
    "    print(count/test_pre.shape[0])\n",
    "\n",
    "count = 0\n",
    "if not test_only:\n",
    "    re_path = data_path+'train_pairs_1000number_idf.csv'\n",
    "    train_recall = get_recall(re_path, 'idf', False)\n",
    "    train_data_idf = train_pre.merge(train_recall, on='description_id', how='inner')\n",
    "    print(count/train_pre.shape[0])\n",
    "\n",
    "# s2v\n",
    "print('recall s2v...')\n",
    "count = 0\n",
    "if not train_only:\n",
    "    re_path = data_path+'test_pairs_1000number_s2v.csv'\n",
    "    test_recall = get_recall(re_path, 's2v', drop_no_content)\n",
    "    test_data_s2v = test_pre.merge(test_recall, on='description_id', how='inner')\n",
    "    print(count/test_pre.shape[0])\n",
    "\n",
    "count = 0\n",
    "if not test_only:\n",
    "    re_path = data_path+'train_pairs_1000number_s2v.csv'\n",
    "    train_recall = get_recall(re_path, 's2v', False)\n",
    "    train_data_s2v = train_pre.merge(train_recall, on='description_id', how='inner')\n",
    "    print(count/train_pre.shape[0])\n",
    "    \n",
    "# blue\n",
    "print('recall blue...')\n",
    "count = 0\n",
    "if not train_only:\n",
    "    re_path = data_path+'test_pairs_1000number_bluebert.csv'\n",
    "    test_recall = get_recall(re_path, 'blue', drop_no_content)\n",
    "    test_data_blue = test_pre.merge(test_recall, on='description_id', how='inner')\n",
    "    print(count/test_pre.shape[0])\n",
    "\n",
    "count = 0\n",
    "if not test_only:\n",
    "    re_path = data_path+'train_pairs_1000number_bluebert.csv'\n",
    "    train_recall = get_recall(re_path, 'blue', False)\n",
    "    train_data_blue = train_pre.merge(train_recall, on='description_id', how='inner')\n",
    "    print(count/train_pre.shape[0])\n",
    "\n",
    "# key\n",
    "print('recall key...')\n",
    "count = 0\n",
    "if not train_only:\n",
    "    re_path = data_path+'test_pairs_500number_key.csv'\n",
    "    test_recall = get_recall(re_path, 'key', drop_no_content)\n",
    "    test_data_key = test_pre.merge(test_recall, on='description_id', how='inner')\n",
    "    print(count/test_pre.shape[0])\n",
    "\n",
    "count = 0\n",
    "if not test_only:\n",
    "    re_path = data_path+'train_pairs_500number_key.csv'\n",
    "    train_recall = get_recall(re_path, 'key', False)\n",
    "    train_data_key = train_pre.merge(train_recall, on='description_id', how='inner')\n",
    "    print(count/train_pre.shape[0])\n",
    "\n",
    "# concat\n",
    "train_data = None\n",
    "test_data = None\n",
    "\n",
    "if not test_only:\n",
    "    train_data = pd.concat([train_data_bm25, train_data_idf, train_data_s2v, train_data_blue, train_data_key],\n",
    "                           axis=0,\n",
    "                           sort=True)\n",
    "    train_data['label'] = list(map(lambda x,y: int(x==y),train_data['pid'],train_data['paper_id']))\n",
    "    print(train_data.shape)\n",
    "    \n",
    "if not train_only:\n",
    "    test_data = pd.concat([test_data_bm25, test_data_idf, test_data_s2v, test_data_blue, test_data_key],\n",
    "                          axis=0,\n",
    "                          sort=True)\n",
    "    print(test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2079282, 6)\n",
      "(2079282, 13)\n",
      "(2079282, 13)\n"
     ]
    }
   ],
   "source": [
    "# drop duplicates\n",
    "if not test_only:\n",
    "    train_data = train_data.drop_duplicates()\n",
    "    print(train_data.shape)\n",
    "if not train_only:\n",
    "    test_data = test_data.drop_duplicates()\n",
    "    print(test_data.shape)\n",
    "    \n",
    "# merge candidate\n",
    "if not test_only:\n",
    "    train_data['paper_id'] = train_data['pid'].values\n",
    "    train_data.pop('pid')\n",
    "    train_data = train_data.merge(candidate_pre, on='paper_id', how='left')\n",
    "    print(train_data.shape)\n",
    "    train_data = train_data[train_data['paper_id'].notnull()]\n",
    "    print(train_data.shape)\n",
    "    print('recall:', sum(train_data['label'].values)/len(train_data['description_id'].unique()))\n",
    "\n",
    "if not train_only:\n",
    "    test_data['paper_id'] = test_data['pid'].values\n",
    "    test_data.pop('pid')\n",
    "    test_data = test_data.merge(candidate_pre, on='paper_id', how='left')\n",
    "    print(test_data.shape)\n",
    "    test_data = test_data[test_data['paper_id'].notnull()]\n",
    "    print(test_data.shape)\n",
    "\n",
    "del candidate_pre\n",
    "gc.collect()\n",
    "\n",
    "# journal != 'no-content'\n",
    "if not test_only:\n",
    "    dids = train_data['description_id'].unique()\n",
    "    did2pid = {row[0]: row[1] for row in train_pre[['description_id', 'paper_id']].values}\n",
    "    have_content_ids = []\n",
    "\n",
    "    for did in dids:\n",
    "        if pid2journal[did2pid[did]] != 'no-content':\n",
    "            have_content_ids.append(did)\n",
    "\n",
    "    tmp = train_data[train_data['description_id'].isin(have_content_ids)]\n",
    "    print('recall (have content):', sum(tmp['label'].values)/len(tmp['description_id'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ns = {'bm25': 50, 'idf': 10, 's2v': 20, 'blue': 5, 'key': 2}\n",
    "# recall = 0.60436\n",
    "# 1/0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2079282, 13)\n"
     ]
    }
   ],
   "source": [
    "# drop description_id that all labels are 0\n",
    "if not test_only:\n",
    "    temp = train_data[['description_id','label']].groupby('description_id')['label'].apply(lambda x:len(set(list(x)))).reset_index()\n",
    "    temp.columns = ['description_id','label']\n",
    "    ids = temp[temp['label'] == 2]['description_id'].values\n",
    "    train_data = train_data[train_data['description_id'].isin(ids)]\n",
    "    \n",
    "# finally write to file\n",
    "if not test_only:\n",
    "    train_data.to_csv(data_path+'train_data_merge_{}.csv'.format(n), index=False)\n",
    "    print(train_data.shape)\n",
    "    print(train_data.label.value_counts(True))\n",
    "if not train_only:\n",
    "    # there are fucking missing values in 'description_id'...\n",
    "    test_data['description_id'] = test_data['description_id'].fillna('none')\n",
    "    test_data.to_csv(data_path+'test_data_merge_{}_{}.csv'.format(n, paper_thd), index=False)\n",
    "    print(test_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get features\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dictionary = corpora.Dictionary.load(other_path+'train_dictionary.dict')\n",
    "tfidf = models.TfidfModel.load(other_path+\"train_idf.model\")\n",
    "word2idf = {tfidf.id2word.id2token[k]: v for k, v in tfidf.idfs.items() if v > 10}\n",
    "index = similarities.SparseMatrixSimilarity.load(other_path+'train_index.index')\n",
    "item_id_list = joblib.load(other_path+'paper_id.pkl')\n",
    "with open(other_path+'train_content.pkl', 'rb') as fr:\n",
    "    corpus = pickle.load(fr)\n",
    "\n",
    "# bert embeddings\n",
    "# # SciBERT\n",
    "with open(other_path+'paper2embedding.pkl', 'rb') as f:\n",
    "    paper2embedding = pickle.load(f)\n",
    "with open(other_path+'description2embedding.pkl', 'rb') as f:\n",
    "    description2embedding = pickle.load(f)\n",
    "# # BlueBERT actually :-p\n",
    "with open(other_path+'paper2embedding_blue.pkl', 'rb') as f:\n",
    "    paper2embedding_bio = pickle.load(f)\n",
    "with open(other_path+'description2embedding_blue.pkl', 'rb') as f:\n",
    "    description2embedding_bio = pickle.load(f)\n",
    "# # sent2vec\n",
    "with open(other_path+'paper2embedding_s2v.pkl', 'rb') as f:\n",
    "    paper2embedding_s2v = pickle.load(f)\n",
    "with open(other_path+'description2embedding_s2v.pkl', 'rb') as f:\n",
    "    description2embedding_s2v = pickle.load(f)\n",
    "# # pre-trained SciBERT\n",
    "with open(other_path+'paper2embedding_pre.pkl', 'rb') as f:\n",
    "    paper2embedding_pre = pickle.load(f)\n",
    "with open(other_path+'description2embedding_pre.pkl', 'rb') as f:\n",
    "    description2embedding_pre = pickle.load(f)\n",
    "    \n",
    "# bm25\n",
    "bm25Model = BM25(corpus)\n",
    "\n",
    "del corpus\n",
    "gc.collect()\n",
    "\n",
    "##################################features works##########################################################\n",
    "\n",
    "def get_features(data_or, postpostfix, vec_model):\n",
    "    def get_df_grams(train_sample, values, cols):\n",
    "        def create_ngram_set(input_list, ngram_value=2):\n",
    "            return set(zip(*[input_list[i:] for i in range(ngram_value)]))\n",
    "\n",
    "        def get_n_gram(df, values=2):\n",
    "            train_query = df.values\n",
    "            train_query = [[word for word in str(sen).replace(\"'\", '').split(' ')] for sen in train_query]\n",
    "            train_query_n = []\n",
    "            for input_list in train_query:\n",
    "                train_query_n_gram = set()\n",
    "                for value in range(2, values + 1):\n",
    "                    train_query_n_gram = train_query_n_gram | create_ngram_set(input_list, value)\n",
    "                train_query_n.append(train_query_n_gram)\n",
    "            return train_query_n\n",
    "\n",
    "        train_query = get_n_gram(train_sample[cols[0]], values)\n",
    "        train_title = get_n_gram(train_sample[cols[1]], values)\n",
    "        sim = list(map(lambda x, y: len(x) + len(y) - 2 * len(x & y),\n",
    "                           train_query, train_title))\n",
    "        sim_number_rate = list(map(lambda x, y: len(x&y) / (len(x)+1e-10) if len(x)!=0 else 0,\n",
    "                           train_query, train_title))\n",
    "        return sim, sim_number_rate\n",
    "\n",
    "    def get_num_key(x, y):\n",
    "        if y == '':\n",
    "            return -1\n",
    "        y = y.strip(';').split(';')\n",
    "        num = 0\n",
    "        for i in y:\n",
    "            if i in x:\n",
    "                num += 1\n",
    "        return num\n",
    "\n",
    "    def get_num_common_words_and_ratio(merge, col):\n",
    "        # merge data\n",
    "        merge = merge[col]\n",
    "        merge.columns = ['q1', 'q2']\n",
    "\n",
    "        q1_word_set = merge.q1.apply(lambda x: x.split(' ')).apply(set).values\n",
    "        q2_word_set = merge.q2.apply(lambda x: x.split(' ')).apply(set).values\n",
    "\n",
    "        q1_word_len = merge.q1.apply(lambda x: len(x.split(' '))).values\n",
    "        q2_word_len = merge.q2.apply(lambda x: len(x.split(' '))).values\n",
    "\n",
    "        q1_word_len_set = merge.q1.apply(lambda x: len(set(x.split(' ')))).values\n",
    "        q2_word_len_set = merge.q2.apply(lambda x: len(set(x.split(' ')))).values\n",
    "\n",
    "        result = [len(q1_word_set[i] & q2_word_set[i]) for i in range(len(q1_word_set))]\n",
    "        result_ratio_q = [result[i] / (q1_word_len[i]+1e-10) for i in range(len(q1_word_set))]\n",
    "        result_ratio_t = [result[i] / (q2_word_len[i]+1e-10) for i in range(len(q1_word_set))]\n",
    "\n",
    "        result_ratio_q_set = [result[i] / (q1_word_len_set[i]+1e-10) for i in range(len(q1_word_set))]\n",
    "        result_ratio_t_set = [result[i] / (q2_word_len_set[i]+1e-10) for i in range(len(q1_word_set))]\n",
    "\n",
    "        return result, result_ratio_q, result_ratio_t, q1_word_len, q2_word_len, q1_word_len_set, q2_word_len_set, result_ratio_q_set, result_ratio_t_set\n",
    "\n",
    "    def jaccard(x, y):\n",
    "        x = set(x)\n",
    "        y = set(y)\n",
    "        return float(len(x & y) / (len(x | y)+1e-10))\n",
    "\n",
    "    def get_sim(doc, corpus):\n",
    "        corpus = corpus.split(' ')\n",
    "        corpus_vec = [dictionary.doc2bow(corpus)]\n",
    "        corpus_tfidf = tfidf[corpus_vec]\n",
    "        featurenum = len(dictionary.token2id.keys())\n",
    "        index_i = similarities.SparseMatrixSimilarity(corpus_tfidf, num_features=featurenum)\n",
    "        doc = doc.split(' ')\n",
    "        vec = dictionary.doc2bow(doc)\n",
    "        vec_tfidf = tfidf[vec]\n",
    "        sim = index_i.get_similarities(vec_tfidf)\n",
    "        return sim[0]\n",
    "\n",
    "    # tfidf\n",
    "    def get_simlilary(query, title):\n",
    "        def get_weight_counter_and_tf_idf(x, y):\n",
    "            x = x.split()\n",
    "            y = y.split()\n",
    "            corups = x + y\n",
    "            obj = dict(collections.Counter(corups))\n",
    "            x_weight = []\n",
    "            y_weight = []\n",
    "            idfs = []\n",
    "            for key in obj.keys():\n",
    "                idf = 1\n",
    "                w = obj[key]\n",
    "                if key in x:\n",
    "                    idf += 1\n",
    "                    x_weight.append(w)\n",
    "                else:\n",
    "                    x_weight.append(0)\n",
    "                if key in y:\n",
    "                    idf += 1\n",
    "                    y_weight.append(w)\n",
    "                else:\n",
    "                    y_weight.append(0)\n",
    "                idfs.append(math.log(3.0 / idf) + 1)\n",
    "            return [np.array(x_weight), np.array(y_weight), np.array(x_weight) * np.array(idfs),\n",
    "                    np.array(y_weight) * np.array(idfs), np.array(list(obj.keys()))]\n",
    "\n",
    "        weight = list(map(lambda x, y: get_weight_counter_and_tf_idf(x, y),\n",
    "                          query, title))\n",
    "        x_weight_couner = []\n",
    "        y_weight_couner = []\n",
    "        x_weight_tfidf = []\n",
    "        y_weight_tfidf = []\n",
    "        words = []\n",
    "        for i in weight:\n",
    "            x_weight_couner.append(i[0])\n",
    "            y_weight_couner.append(i[1])\n",
    "            x_weight_tfidf.append(i[2])\n",
    "            y_weight_tfidf.append(i[3])\n",
    "            words.append(i[4])\n",
    "\n",
    "        def mhd_simlilary(x, y):\n",
    "            return np.linalg.norm(x - y, ord=1)\n",
    "\n",
    "        mhd_simlilary_counter = list(map(lambda x, y: mhd_simlilary(x, y),\n",
    "                                         x_weight_couner, y_weight_couner))\n",
    "        mhd_simlilary_tfidf = list(map(lambda x, y: mhd_simlilary(x, y),\n",
    "                                       x_weight_tfidf, y_weight_tfidf))\n",
    "\n",
    "        def cos_simlilary(x, y):\n",
    "            return np.dot(x, y) / (np.linalg.norm(x) * np.linalg.norm(y))\n",
    "\n",
    "        cos_simlilary_counter = list(map(lambda x, y: cos_simlilary(x, y),\n",
    "                                         x_weight_couner, y_weight_couner))\n",
    "        cos_simlilary_tfidf = list(map(lambda x, y: cos_simlilary(x, y),\n",
    "                                       x_weight_tfidf, y_weight_tfidf))\n",
    "\n",
    "        def Euclidean_simlilary(x, y):\n",
    "            return np.sqrt(np.sum(x - y) ** 2)\n",
    "\n",
    "        Euclidean_simlilary_counter = list(map(lambda x, y: Euclidean_simlilary(x, y),\n",
    "                                               x_weight_couner, y_weight_couner))\n",
    "        Euclidean__simlilary_tfidf = list(map(lambda x, y: Euclidean_simlilary(x, y),\n",
    "                                              x_weight_tfidf, y_weight_tfidf))\n",
    "\n",
    "        return mhd_simlilary_counter, mhd_simlilary_tfidf, cos_simlilary_counter, \\\n",
    "               cos_simlilary_tfidf, Euclidean_simlilary_counter, Euclidean__simlilary_tfidf\n",
    "\n",
    "    def get_vec(x):\n",
    "        vec = []\n",
    "        for word in x.split():\n",
    "            if word in vec_model:\n",
    "                vec.append(vec_model[word])\n",
    "        if len(vec) == 0:\n",
    "            return np.nan\n",
    "        else:\n",
    "            return np.mean(np.array(vec), axis=0)\n",
    "        \n",
    "    def get_vec_ft(x):\n",
    "        vec = [fasttext_model[word] for word in x.split()]\n",
    "        if len(vec) == 0:\n",
    "            return np.nan\n",
    "        else:\n",
    "            return np.mean(np.array(vec), axis=0)\n",
    "    \n",
    "    def get_vec_sif(x, postfix):\n",
    "        return sif_model.sv[tag2idx[x+postfix+postpostfix]]\n",
    "    \n",
    "    def get_vec_sif_paper(x, postfix):\n",
    "        return sif_model.sv[tag2idx[x+postfix]]\n",
    "\n",
    "    # get bm25\n",
    "    def get_bm25(p_id, query):\n",
    "        query = str(query).split(' ')\n",
    "        score = bm25Model.get_score(query, item_id_list.index(p_id))\n",
    "        return score\n",
    "\n",
    "    def apply_fun(df):\n",
    "        df.columns = ['d_id', 'key', 'doc']\n",
    "        query_id_group = df.groupby(['d_id'])\n",
    "        bm_list = []\n",
    "        for name, group in query_id_group:\n",
    "            corpus = group['doc'].values.tolist()\n",
    "            corpus = [sentence.strip().split() if sentence != '' else ['none'] \\\n",
    "                      for sentence in corpus]\n",
    "            query = group['key'].values[0].strip().split()\n",
    "            bm25Model = BM25(corpus)\n",
    "            bmscore = bm25Model.get_scores(query)\n",
    "            bm_list.extend(bmscore)\n",
    "        return bm_list\n",
    "    \n",
    "    def get_bert_embedding(id, take):\n",
    "        if take == 'abstract' or take == 'title':\n",
    "            return paper2embedding[id][take]\n",
    "        else:\n",
    "            return description2embedding[id+postpostfix][take]\n",
    "        \n",
    "    def get_bert_embedding_bio(id, take):\n",
    "        if take == 'abstract' or take == 'title':\n",
    "            return paper2embedding_bio[id][take]\n",
    "        else:\n",
    "            return description2embedding_bio[id+postpostfix][take]\n",
    "        \n",
    "    def get_bert_embedding_s2v(id, take):\n",
    "        if take == 'abstract' or take == 'title':\n",
    "            return paper2embedding_s2v[id][take]\n",
    "        else:\n",
    "            return description2embedding_s2v[id+postpostfix][take]\n",
    "        \n",
    "    def get_bert_embedding_pre(id, take):\n",
    "        if take == 'title':\n",
    "            return paper2embedding_pre[id]\n",
    "        else:\n",
    "            return description2embedding_pre[id+postpostfix]\n",
    "        \n",
    "    def get_num_upper(x, y, rate=True):\n",
    "        if y == '':\n",
    "            return -1\n",
    "        x = [word for word in x.split() if word.isupper() and word != 'A']\n",
    "        y = [word for word in y.split() if word.isupper() and word != 'A']\n",
    "        set_y = set(y)\n",
    "        ret = 0\n",
    "        for word in x:\n",
    "            if word in set_y:\n",
    "                ret += 1\n",
    "        ret = ret/(len(y)+1e-10) if rate else ret\n",
    "        return ret\n",
    "    \n",
    "    def get_num_psu_key(x, y, thd, rate=True):\n",
    "        def filter_len(string):\n",
    "            ret = [word for word in string.split() if len(word) >= MIN_LEN and word in word2idf]\n",
    "            ret = [word for word in ret if word2idf[word] > thd]\n",
    "            return ret\n",
    "\n",
    "        def top_k(string, k=10):\n",
    "            ret = sorted(string.split(), key=lambda x: len(x), reverse=True)[:k]\n",
    "            ret = [kw for kw in ret if len(kw) >= 8 and kw in word2idf]\n",
    "            ret = [kw for kw in ret if word2idf[kw] > thd]\n",
    "            return ret\n",
    "        \n",
    "        if y == '':\n",
    "            return -1\n",
    "        x = filter_len(x)\n",
    "        y = top_k(y)\n",
    "        set_y = set(y)\n",
    "        ret = 0\n",
    "        for word in x:\n",
    "            if word in set_y:\n",
    "                ret += 1\n",
    "        ret = ret/(len(y)+1e-10) if rate else ret\n",
    "        return ret\n",
    "    \n",
    "    data = data_or.copy()\n",
    "\n",
    "    data['key_text_pre'] = data['key_text_pre'].apply(\n",
    "        lambda x: x.replace('.', ' '))\n",
    "    \n",
    "    data['abstract_pre'] = data['abstract_pre'].apply(\n",
    "        lambda x: '' if len(x) < 9 else x)\n",
    "\n",
    "    data['abstract_pre'] = data['abstract_pre'].apply(\n",
    "        lambda x: '' if x.split(' ') == ['n', 'o', 'n', 'e'] else x)\n",
    "    \n",
    "    prefix = 'num_'\n",
    "    data[prefix + 'key_text_len'] = data['key_text_pre'].apply(lambda x: len(x.split(' ')))\n",
    "    data[prefix + 'description_text_pre_len'] = data['description_text_pre'].apply(lambda x: len(x.split(' ')))\n",
    "\n",
    "    data.loc[data[prefix + 'key_text_len'] < MIN_LEN, 'key_text_pre'] = data[data[prefix + 'key_text_len'] < MIN_LEN][\n",
    "        'description_text'].apply(\n",
    "        lambda x: ' '.join(pre_process(re.sub(r'[\\[|,]+\\*\\*\\#\\#\\*\\*[\\]|,]+', '', x)))).values\n",
    "\n",
    "    # psu_key\n",
    "    # # 10\n",
    "    data[prefix + 'key_in_psu_key_number_rate_kt_10'] = list(map(lambda x,y: get_num_psu_key(x,y,10),data['key_text_pre'],data['title_pro']))\n",
    "    data[prefix + 'key_in_psu_key_number_rate_ka_10'] = list(map(lambda x,y: get_num_psu_key(x,y,10),data['key_text_pre'],data['abstract_pre']))\n",
    "    data[prefix + 'key_in_psu_key_number_rate_dt_10'] = list(map(lambda x,y: get_num_psu_key(x,y,10),data['description_text_pre'],data['title_pro']))\n",
    "    data[prefix + 'key_in_psu_key_number_rate_da_10'] = list(map(lambda x,y: get_num_psu_key(x,y,10),data['description_text_pre'],data['abstract_pre']))\n",
    "    \n",
    "    data[prefix + 'key_in_psu_key_number_kt_10'] = list(map(lambda x,y: get_num_psu_key(x,y,10,False),data['key_text_pre'],data['title_pro']))\n",
    "    data[prefix + 'key_in_psu_key_number_ka_10'] = list(map(lambda x,y: get_num_psu_key(x,y,10,False),data['key_text_pre'],data['abstract_pre']))\n",
    "    data[prefix + 'key_in_psu_key_number_dt_10'] = list(map(lambda x,y: get_num_psu_key(x,y,10,False),data['description_text_pre'],data['title_pro']))\n",
    "    data[prefix + 'key_in_psu_key_number_da_10'] = list(map(lambda x,y: get_num_psu_key(x,y,10,False),data['description_text_pre'],data['abstract_pre']))\n",
    "    # # 11\n",
    "    data[prefix + 'key_in_psu_key_number_rate_kt_11'] = list(map(lambda x,y: get_num_psu_key(x,y,11),data['key_text_pre'],data['title_pro']))\n",
    "    data[prefix + 'key_in_psu_key_number_rate_ka_11'] = list(map(lambda x,y: get_num_psu_key(x,y,11),data['key_text_pre'],data['abstract_pre']))\n",
    "    data[prefix + 'key_in_psu_key_number_rate_dt_11'] = list(map(lambda x,y: get_num_psu_key(x,y,11),data['description_text_pre'],data['title_pro']))\n",
    "    data[prefix + 'key_in_psu_key_number_rate_da_11'] = list(map(lambda x,y: get_num_psu_key(x,y,11),data['description_text_pre'],data['abstract_pre']))\n",
    "    \n",
    "    data[prefix + 'key_in_psu_key_number_kt_11'] = list(map(lambda x,y: get_num_psu_key(x,y,11,False),data['key_text_pre'],data['title_pro']))\n",
    "    data[prefix + 'key_in_psu_key_number_ka_11'] = list(map(lambda x,y: get_num_psu_key(x,y,11,False),data['key_text_pre'],data['abstract_pre']))\n",
    "    data[prefix + 'key_in_psu_key_number_dt_11'] = list(map(lambda x,y: get_num_psu_key(x,y,11,False),data['description_text_pre'],data['title_pro']))\n",
    "    data[prefix + 'key_in_psu_key_number_da_11'] = list(map(lambda x,y: get_num_psu_key(x,y,11,False),data['description_text_pre'],data['abstract_pre']))\n",
    "    # # 12\n",
    "    data[prefix + 'key_in_psu_key_number_rate_kt_12'] = list(map(lambda x,y: get_num_psu_key(x,y,12),data['key_text_pre'],data['title_pro']))\n",
    "    data[prefix + 'key_in_psu_key_number_rate_ka_12'] = list(map(lambda x,y: get_num_psu_key(x,y,12),data['key_text_pre'],data['abstract_pre']))\n",
    "    data[prefix + 'key_in_psu_key_number_rate_dt_12'] = list(map(lambda x,y: get_num_psu_key(x,y,12),data['description_text_pre'],data['title_pro']))\n",
    "    data[prefix + 'key_in_psu_key_number_rate_da_12'] = list(map(lambda x,y: get_num_psu_key(x,y,12),data['description_text_pre'],data['abstract_pre']))\n",
    "    \n",
    "    data[prefix + 'key_in_psu_key_number_kt_12'] = list(map(lambda x,y: get_num_psu_key(x,y,12,False),data['key_text_pre'],data['title_pro']))\n",
    "    data[prefix + 'key_in_psu_key_number_ka_12'] = list(map(lambda x,y: get_num_psu_key(x,y,12,False),data['key_text_pre'],data['abstract_pre']))\n",
    "    data[prefix + 'key_in_psu_key_number_dt_12'] = list(map(lambda x,y: get_num_psu_key(x,y,12,False),data['description_text_pre'],data['title_pro']))\n",
    "    data[prefix + 'key_in_psu_key_number_da_12'] = list(map(lambda x,y: get_num_psu_key(x,y,12,False),data['description_text_pre'],data['abstract_pre']))\n",
    "    # # 13\n",
    "    data[prefix + 'key_in_psu_key_number_rate_kt_13'] = list(map(lambda x,y: get_num_psu_key(x,y,13),data['key_text_pre'],data['title_pro']))\n",
    "    data[prefix + 'key_in_psu_key_number_rate_ka_13'] = list(map(lambda x,y: get_num_psu_key(x,y,13),data['key_text_pre'],data['abstract_pre']))\n",
    "    data[prefix + 'key_in_psu_key_number_rate_dt_13'] = list(map(lambda x,y: get_num_psu_key(x,y,13),data['description_text_pre'],data['title_pro']))\n",
    "    data[prefix + 'key_in_psu_key_number_rate_da_13'] = list(map(lambda x,y: get_num_psu_key(x,y,13),data['description_text_pre'],data['abstract_pre']))\n",
    "    \n",
    "    data[prefix + 'key_in_psu_key_number_kt_13'] = list(map(lambda x,y: get_num_psu_key(x,y,13,False),data['key_text_pre'],data['title_pro']))\n",
    "    data[prefix + 'key_in_psu_key_number_ka_13'] = list(map(lambda x,y: get_num_psu_key(x,y,13,False),data['key_text_pre'],data['abstract_pre']))\n",
    "    data[prefix + 'key_in_psu_key_number_dt_13'] = list(map(lambda x,y: get_num_psu_key(x,y,13,False),data['description_text_pre'],data['title_pro']))\n",
    "    data[prefix + 'key_in_psu_key_number_da_13'] = list(map(lambda x,y: get_num_psu_key(x,y,13,False),data['description_text_pre'],data['abstract_pre']))\n",
    "    # # 14\n",
    "    data[prefix + 'key_in_psu_key_number_rate_kt_14'] = list(map(lambda x,y: get_num_psu_key(x,y,14),data['key_text_pre'],data['title_pro']))\n",
    "    data[prefix + 'key_in_psu_key_number_rate_ka_14'] = list(map(lambda x,y: get_num_psu_key(x,y,14),data['key_text_pre'],data['abstract_pre']))\n",
    "    data[prefix + 'key_in_psu_key_number_rate_dt_14'] = list(map(lambda x,y: get_num_psu_key(x,y,14),data['description_text_pre'],data['title_pro']))\n",
    "    data[prefix + 'key_in_psu_key_number_rate_da_14'] = list(map(lambda x,y: get_num_psu_key(x,y,14),data['description_text_pre'],data['abstract_pre']))\n",
    "    \n",
    "    data[prefix + 'key_in_psu_key_number_kt_14'] = list(map(lambda x,y: get_num_psu_key(x,y,14,False),data['key_text_pre'],data['title_pro']))\n",
    "    data[prefix + 'key_in_psu_key_number_ka_14'] = list(map(lambda x,y: get_num_psu_key(x,y,14,False),data['key_text_pre'],data['abstract_pre']))\n",
    "    data[prefix + 'key_in_psu_key_number_dt_14'] = list(map(lambda x,y: get_num_psu_key(x,y,14,False),data['description_text_pre'],data['title_pro']))\n",
    "    data[prefix + 'key_in_psu_key_number_da_14'] = list(map(lambda x,y: get_num_psu_key(x,y,14,False),data['description_text_pre'],data['abstract_pre']))\n",
    "    \n",
    "    # upper\n",
    "    data[prefix + 'key_in_upper_number_rate_kt'] = list(map(lambda x,y: get_num_upper(x,y),data['key_text_pre'],data['title_pro']))\n",
    "    data[prefix + 'key_in_upper_number_rate_ka'] = list(map(lambda x,y: get_num_upper(x,y),data['key_text_pre'],data['abstract_pre']))\n",
    "    data[prefix + 'key_in_upper_number_rate_dt'] = list(map(lambda x,y: get_num_upper(x,y),data['description_text_pre'],data['title_pro']))\n",
    "    data[prefix + 'key_in_upper_number_rate_da'] = list(map(lambda x,y: get_num_upper(x,y),data['description_text_pre'],data['abstract_pre']))\n",
    "    \n",
    "    data[prefix + 'key_in_upper_number_kt'] = list(map(lambda x,y: get_num_upper(x,y,False),data['key_text_pre'],data['title_pro']))\n",
    "    data[prefix + 'key_in_upper_number_ka'] = list(map(lambda x,y: get_num_upper(x,y,False),data['key_text_pre'],data['abstract_pre']))\n",
    "    data[prefix + 'key_in_upper_number_dt'] = list(map(lambda x,y: get_num_upper(x,y,False),data['description_text_pre'],data['title_pro']))\n",
    "    data[prefix + 'key_in_upper_number_da'] = list(map(lambda x,y: get_num_upper(x,y,False),data['description_text_pre'],data['abstract_pre']))\n",
    "    \n",
    "    # sent2vec\n",
    "    data['key_text_s2v'] = data['description_id'].apply(lambda x: get_bert_embedding_s2v(x, 'key_text'))\n",
    "    data['title_s2v'] = data['paper_id'].apply(lambda x: get_bert_embedding_s2v(x, 'title'))\n",
    "    data['abstract_s2v'] = data['paper_id'].apply(lambda x: get_bert_embedding_s2v(x, 'abstract'))\n",
    "    data['description_text_s2v'] = data['description_id'].apply(lambda x: get_bert_embedding_s2v(x, 'description_text'))\n",
    "    # pretrained bert\n",
    "    data['title_bert_pre'] = data['paper_id'].apply(lambda x: get_bert_embedding_pre(x, 'title'))\n",
    "    data['description_text_bert_pre'] = data['description_id'].apply(lambda x: get_bert_embedding_pre(x, 'description_text'))\n",
    "    # sif\n",
    "    data['key_text_pre_sif'] = data['description_id'].apply(lambda x: get_vec_sif(x, '_key'))\n",
    "    data['title_pro_sif'] = data['paper_id'].apply(lambda x: get_vec_sif_paper(x, '_title'))\n",
    "    data['abstract_pre_sif'] = data['paper_id'].apply(lambda x: get_vec_sif_paper(x, '_abstract'))\n",
    "    data['description_text_pre_sif'] = data['description_id'].apply(lambda x: get_vec_sif(x, '_description'))\n",
    "    # word2vec\n",
    "    data['key_text_pre_vec'] = data['key_text_pre'].apply(lambda x: get_vec(x))\n",
    "    data['title_pro_vec'] = data['title_pro'].apply(lambda x: get_vec(x))\n",
    "    data['abstract_pre_vec'] = data['abstract_pre'].apply(lambda x: get_vec(x))\n",
    "    data['description_text_pre_vec'] = data['description_text_pre'].apply(lambda x: get_vec(x))\n",
    "    # fasttext\n",
    "    data['key_text_pre_fasttext'] = data['key_text_pre'].apply(lambda x: get_vec_ft(x))\n",
    "    data['title_pro_fasttext'] = data['title_pro'].apply(lambda x: get_vec_ft(x))\n",
    "    data['abstract_pre_fasttext'] = data['abstract_pre'].apply(lambda x: get_vec_ft(x))\n",
    "    data['description_text_pre_fasttext'] = data['description_text_pre'].apply(lambda x: get_vec_ft(x))\n",
    "    # scibert\n",
    "    data['key_text_bert'] = data['description_id'].apply(lambda x: get_bert_embedding(x, 'key_text'))\n",
    "    data['title_bert'] = data['paper_id'].apply(lambda x: get_bert_embedding(x, 'title'))\n",
    "    data['abstract_bert'] = data['paper_id'].apply(lambda x: get_bert_embedding(x, 'abstract'))\n",
    "    data['description_text_bert'] = data['description_id'].apply(lambda x: get_bert_embedding(x, 'description_text'))\n",
    "    # biobert\n",
    "    data['key_text_biobert'] = data['description_id'].apply(lambda x: get_bert_embedding_bio(x, 'key_text'))\n",
    "    data['title_biobert'] = data['paper_id'].apply(lambda x: get_bert_embedding_bio(x, 'title'))\n",
    "    data['abstract_biobert'] = data['paper_id'].apply(lambda x: get_bert_embedding_bio(x, 'abstract'))\n",
    "    data['description_text_biobert'] = data['description_id'].apply(lambda x: get_bert_embedding_bio(x, 'description_text'))\n",
    "    \n",
    "    data[prefix + 'cate_pa_isnull'] = data['abstract_pre'].apply(lambda x: 1 if x == '' else 0)\n",
    "    data[prefix + 'cate_pkeywords_isnull'] = data['keywords'].apply(lambda x: 1 if x == '' else 0)\n",
    "\n",
    "    data[prefix + 'key_in_key_word_number'] = list(map(lambda x,y: get_num_key(x,y),data['key_text_pre'],data['keywords']))\n",
    "    data[prefix + 'key_in_key_word_number_rate'] = list(map(lambda x,y: 0 if x==-1 else x/(len(y.strip(';').split(';'))+1e-10),data[prefix+'key_in_key_word_number'],\n",
    "                                                data['keywords']))\n",
    "    \n",
    "    #append\n",
    "    data[prefix + 'key_in_key_word_number2'] = list(map(lambda x,y: get_num_key(x,y),data['description_text_pre'],data['keywords']))\n",
    "    data[prefix + 'key_in_key_word_number2_rate'] = list(map(lambda x,y: 0 if x==-1 else x/(len(y.strip(';').split(';'))+1e-10),data[prefix+'key_in_key_word_number2'],\n",
    "                                                data['keywords']))\n",
    "    \n",
    "    data[prefix + 'common_words_k_pt'], \\\n",
    "    data[prefix + 'common_words_k_pt_k'], \\\n",
    "    data[prefix + 'common_words_k_pt_pt'], \\\n",
    "    data[prefix + 'k_len'], \\\n",
    "    data[prefix + 'pt_len'], \\\n",
    "    data[prefix + 'k_len_set'], \\\n",
    "    data[prefix + 'pt_len_set'], \\\n",
    "    data[prefix + 'common_words_k_pt_k_set'], \\\n",
    "    data[prefix + 'common_words_k_pt_pt_set'] = get_num_common_words_and_ratio(data, ['key_text_pre', 'title_pro'])\n",
    "\n",
    "    data[prefix + 'common_words_k_at'], \\\n",
    "    data[prefix + 'common_words_k_at_k'], \\\n",
    "    data[prefix + 'common_words_k_at_at'], \\\n",
    "    data[prefix + 'k_len'], \\\n",
    "    data[prefix + 'at_len'], \\\n",
    "    data[prefix + 'k_len_set'], \\\n",
    "    data[prefix + 'at_len_set'], \\\n",
    "    data[prefix + 'common_words_k_at_k_set'], \\\n",
    "    data[prefix + 'common_words_k_at_at_set'] = get_num_common_words_and_ratio(data, ['key_text_pre', 'abstract_pre'])\n",
    "\n",
    "    #append\n",
    "    data[prefix + 'common_words_k_pt_2'], \\\n",
    "    data[prefix + 'common_words_k_pt_k_2'], \\\n",
    "    data[prefix + 'common_words_k_pt_pt_2'], \\\n",
    "    data[prefix + 'k_len_2'], \\\n",
    "    data[prefix + 'pt_len'], \\\n",
    "    data[prefix + 'k_len_set_2'], \\\n",
    "    data[prefix + 'pt_len_set'], \\\n",
    "    data[prefix + 'common_words_k_pt_k_set_2'], \\\n",
    "    data[prefix + 'common_words_k_pt_pt_set_2'] = get_num_common_words_and_ratio(data, ['description_text_pre', 'title_pro'])\n",
    "\n",
    "    data[prefix + 'common_words_k_at_2'], \\\n",
    "    data[prefix + 'common_words_k_at_k_2'], \\\n",
    "    data[prefix + 'common_words_k_at_at_2'], \\\n",
    "    data[prefix + 'k_len_2'], \\\n",
    "    data[prefix + 'at_len'], \\\n",
    "    data[prefix + 'k_len_set_2'], \\\n",
    "    data[prefix + 'at_len_set'], \\\n",
    "    data[prefix + 'common_words_k_at_k_set_2'], \\\n",
    "    data[prefix + 'common_words_k_at_at_set_2'] = get_num_common_words_and_ratio(data, ['description_text_pre', 'abstract_pre'])\n",
    "\n",
    "    data[prefix + 'jaccard_sim_k_pt'] = list(map(lambda x, y: jaccard(x, y), data['key_text_pre'], data['title_pro']))\n",
    "    data[prefix + 'jaccard_sim_k_pa'] = list(\n",
    "        map(lambda x, y: jaccard(x, y), data['key_text_pre'], data['abstract_pre']))\n",
    "\n",
    "    #append\n",
    "    data[prefix + 'jaccard_sim_k_pt2'] = list(map(lambda x, y: jaccard(x, y), data['description_text_pre'], data['title_pro']))\n",
    "    data[prefix + 'jaccard_sim_k_pa2'] = list(\n",
    "        map(lambda x, y: jaccard(x, y), data['key_text_pre'], data['description_text_pre']))\n",
    "\n",
    "    data[prefix + 'edict_distance_k_pt'] = list(\n",
    "        map(lambda x, y: Levenshtein.distance(x, y) / (len(x)+1e-10), data['key_text_pre'], data['title_pro']))\n",
    "    data[prefix + 'edict_jaro'] = list(\n",
    "        map(lambda x, y: Levenshtein.jaro(x, y), data['key_text_pre'], data['title_pro']))\n",
    "    data[prefix + 'edict_ratio'] = list(\n",
    "        map(lambda x, y: Levenshtein.ratio(x, y), data['key_text_pre'], data['title_pro']))\n",
    "    data[prefix + 'edict_jaro_winkler'] = list(\n",
    "        map(lambda x, y: Levenshtein.jaro_winkler(x, y), data['key_text_pre'], data['title_pro']))\n",
    "\n",
    "    data[prefix + 'edict_distance_k_pa'] = list(\n",
    "        map(lambda x, y: Levenshtein.distance(x, y) / (len(x)+1e-10), data['key_text_pre'],\n",
    "            data['abstract_pre']))\n",
    "    data[prefix + 'edict_jaro_pa'] = list(\n",
    "        map(lambda x, y: Levenshtein.jaro(x, y), data['key_text_pre'], data['abstract_pre']))\n",
    "    data[prefix + 'edict_ratio_pa'] = list(\n",
    "        map(lambda x, y: Levenshtein.ratio(x, y), data['key_text_pre'], data['abstract_pre']))\n",
    "    data[prefix + 'edict_jaro_winkler_pa'] = list(\n",
    "        map(lambda x, y: Levenshtein.jaro_winkler(x, y), data['key_text_pre'], data['abstract_pre']))\n",
    "\n",
    "    #append\n",
    "    data[prefix + 'edict_distance_k_pt_2'] = list(\n",
    "        map(lambda x, y: Levenshtein.distance(x, y) / (len(x)+1e-10), data['description_text_pre'], data['title_pro']))\n",
    "    data[prefix + 'edict_jaro_2'] = list(\n",
    "        map(lambda x, y: Levenshtein.jaro(x, y), data['description_text_pre'], data['title_pro']))\n",
    "    data[prefix + 'edict_ratio_2'] = list(\n",
    "        map(lambda x, y: Levenshtein.ratio(x, y), data['description_text_pre'], data['title_pro']))\n",
    "    data[prefix + 'edict_jaro_winkler_2'] = list(\n",
    "        map(lambda x, y: Levenshtein.jaro_winkler(x, y), data['description_text_pre'], data['title_pro']))\n",
    "\n",
    "    data[prefix + 'edict_distance_k_pa_2'] = list(\n",
    "        map(lambda x, y: Levenshtein.distance(x, y) / (len(x)+1e-10), data['description_text_pre'],\n",
    "            data['abstract_pre']))\n",
    "    data[prefix + 'edict_jaro_pa_2'] = list(\n",
    "        map(lambda x, y: Levenshtein.jaro(x, y), data['description_text_pre'], data['abstract_pre']))\n",
    "    data[prefix + 'edict_ratio_pa_2'] = list(\n",
    "        map(lambda x, y: Levenshtein.ratio(x, y), data['description_text_pre'], data['abstract_pre']))\n",
    "    data[prefix + 'edict_jaro_winkler_pa_2'] = list(\n",
    "        map(lambda x, y: Levenshtein.jaro_winkler(x, y), data['description_text_pre'], data['abstract_pre']))\n",
    "\n",
    "    data[prefix + 'sim'] = list(map(lambda x, y: get_sim(x, y), data['key_text_pre'], data['title_pro']))\n",
    "    data[prefix + 'sim_pa'] = list(map(lambda x, y: get_sim(x, y), data['key_text_pre'], data['abstract_pre']))\n",
    "\n",
    "    #append\n",
    "    data[prefix + 'sim_2'] = list(map(lambda x, y: get_sim(x, y), data['description_text_pre'], data['title_pro']))\n",
    "    data[prefix + 'sim_pa_2'] = list(map(lambda x, y: get_sim(x, y), data['description_text_pre'], data['abstract_pre']))\n",
    "\n",
    "\n",
    "    data[prefix + 'mhd_similiary'], data[prefix + 'tf_mhd_similiary'], \\\n",
    "    data[prefix + 'cos_similiary'], data[prefix + 'tf_cos_similiary'], \\\n",
    "    data[prefix + 'os_similiary'], data[prefix + 'tf_os_similiary'] = get_simlilary(data['key_text_pre'],data['title_pro'])\n",
    "\n",
    "\n",
    "    data[prefix + 'mhd_similiary_pa'], data[prefix + 'tf_mhd_similiary_pa'], \\\n",
    "    data[prefix + 'cos_similiary_pa'], data[prefix + 'tf_cos_similiary_pa'], \\\n",
    "    data[prefix + 'os_similiary_pa'], data[prefix + 'tf_os_similiary_pa'] = get_simlilary(data['key_text_pre'],data['abstract_pre'])\n",
    "    \n",
    "    # cos\n",
    "    data[prefix + 'cos_mean_word2vec'] = list(map(lambda x, y: np.dot(x, y) / (np.linalg.norm(x) * np.linalg.norm(y)),\n",
    "                                                  data['key_text_pre_vec'], data['title_pro_vec']))\n",
    "    data[prefix + 'cos_mean_word2vec'] = data[prefix + 'cos_mean_word2vec'].apply(\n",
    "        lambda x: np.nan if np.isnan(x).any() else x)\n",
    "    data[prefix + 'os_mean_word2vec'] = list(map(lambda x, y: np.sqrt(np.sum((x - y) ** 2)),\n",
    "                                                 data['key_text_pre_vec'], data['title_pro_vec']))\n",
    "    data[prefix + 'cos_mean_fasttext'] = list(map(lambda x, y: np.dot(x, y) / (np.linalg.norm(x) * np.linalg.norm(y)),\n",
    "                                                  data['key_text_pre_fasttext'], data['title_pro_fasttext']))\n",
    "    data[prefix + 'cos_mean_fasttext'] = data[prefix + 'cos_mean_fasttext'].apply(\n",
    "        lambda x: np.nan if np.isnan(x).any() else x)\n",
    "    data[prefix + 'os_mean_fasttext'] = list(map(lambda x, y: np.sqrt(np.sum((x - y) ** 2)),\n",
    "                                                 data['key_text_pre_fasttext'], data['title_pro_fasttext']))\n",
    "    data[prefix + 'cos_mean_sif'] = list(map(lambda x, y: np.dot(x, y) / (np.linalg.norm(x) * np.linalg.norm(y)),\n",
    "                                                  data['key_text_pre_sif'], data['title_pro_sif']))\n",
    "    data[prefix + 'cos_mean_sif'] = data[prefix + 'cos_mean_sif'].apply(\n",
    "        lambda x: np.nan if np.isnan(x).any() else x)\n",
    "    data[prefix + 'os_mean_sif'] = list(map(lambda x, y: np.sqrt(np.sum((x - y) ** 2)),\n",
    "                                                 data['key_text_pre_sif'], data['title_pro_sif']))\n",
    "    data[prefix + 'cos_mean_bert'] = list(map(lambda x, y: np.dot(x, y) / (np.linalg.norm(x) * np.linalg.norm(y)),\n",
    "                                                  data['key_text_bert'], data['title_bert']))\n",
    "    data[prefix + 'cos_mean_bert'] = data[prefix + 'cos_mean_bert'].apply(\n",
    "        lambda x: np.nan if np.isnan(x).any() else x)\n",
    "    data[prefix + 'os_mean_bert'] = list(map(lambda x, y: np.sqrt(np.sum((x - y) ** 2)),\n",
    "                                                 data['key_text_bert'], data['title_bert']))\n",
    "    data[prefix + 'cos_mean_biobert'] = list(map(lambda x, y: np.dot(x, y) / (np.linalg.norm(x) * np.linalg.norm(y)),\n",
    "                                                  data['key_text_biobert'], data['title_biobert']))\n",
    "    data[prefix + 'cos_mean_biobert'] = data[prefix + 'cos_mean_biobert'].apply(\n",
    "        lambda x: np.nan if np.isnan(x).any() else x)\n",
    "    data[prefix + 'os_mean_biobert'] = list(map(lambda x, y: np.sqrt(np.sum((x - y) ** 2)),\n",
    "                                                 data['key_text_biobert'], data['title_biobert']))\n",
    "    data[prefix + 'cos_mean_s2v'] = list(map(lambda x, y: np.dot(x, y) / (np.linalg.norm(x) * np.linalg.norm(y)),\n",
    "                                                  data['key_text_s2v'], data['title_s2v']))\n",
    "    data[prefix + 'cos_mean_s2v'] = data[prefix + 'cos_mean_s2v'].apply(\n",
    "        lambda x: np.nan if np.isnan(x).any() else x)\n",
    "    data[prefix + 'os_mean_s2v'] = list(map(lambda x, y: np.sqrt(np.sum((x - y) ** 2)),\n",
    "                                                 data['key_text_s2v'], data['title_s2v']))\n",
    "\n",
    "    # mhd\n",
    "    data[prefix + 'mhd_mean_word2vec'] = list(map(lambda x, y: np.nan if np.isnan(x).any() or np.isnan(y).any() else\n",
    "    np.linalg.norm(x - y, ord=1), data['key_text_pre_vec'], data['title_pro_vec']))\n",
    "    data[prefix + 'mhd_mean_fasttext'] = list(map(lambda x, y: np.nan if np.isnan(x).any() or np.isnan(y).any() else\n",
    "    np.linalg.norm(x - y, ord=1), data['key_text_pre_fasttext'], data['title_pro_fasttext']))\n",
    "    data[prefix + 'mhd_mean_sif'] = list(map(lambda x, y: np.nan if np.isnan(x).any() or np.isnan(y).any() else\n",
    "    np.linalg.norm(x - y, ord=1), data['key_text_pre_sif'], data['title_pro_sif']))\n",
    "    data[prefix + 'mhd_mean_bert'] = list(map(lambda x, y: np.nan if np.isnan(x).any() or np.isnan(y).any() else\n",
    "    np.linalg.norm(x - y, ord=1), data['key_text_bert'], data['title_bert']))\n",
    "    data[prefix + 'mhd_mean_biobert'] = list(map(lambda x, y: np.nan if np.isnan(x).any() or np.isnan(y).any() else\n",
    "    np.linalg.norm(x - y, ord=1), data['key_text_biobert'], data['title_biobert']))\n",
    "    data[prefix + 'mhd_mean_s2v'] = list(map(lambda x, y: np.nan if np.isnan(x).any() or np.isnan(y).any() else\n",
    "    np.linalg.norm(x - y, ord=1), data['key_text_s2v'], data['title_s2v']))\n",
    "\n",
    "    # cos\n",
    "    data[prefix + 'cos_mean_word2vec_pa'] = list(map(lambda x, y: np.dot(x, y) / (np.linalg.norm(x) * np.linalg.norm(y)),\n",
    "                                                  data['key_text_pre_vec'], data['abstract_pre_vec']))\n",
    "    data[prefix + 'cos_mean_word2vec_pa'] = data[prefix + 'cos_mean_word2vec_pa'].apply(\n",
    "        lambda x: np.nan if np.isnan(x).any() else x)\n",
    "    data[prefix + 'os_mean_word2vec_pa'] = list(map(lambda x, y: np.sqrt(np.sum((x - y) ** 2)),\n",
    "                                                 data['key_text_pre_vec'], data['abstract_pre_vec']))\n",
    "    data[prefix + 'cos_mean_fasttext_pa'] = list(map(lambda x, y: np.dot(x, y) / (np.linalg.norm(x) * np.linalg.norm(y)),\n",
    "                                                  data['key_text_pre_fasttext'], data['abstract_pre_fasttext']))\n",
    "    data[prefix + 'cos_mean_fasttext_pa'] = data[prefix + 'cos_mean_fasttext_pa'].apply(\n",
    "        lambda x: np.nan if np.isnan(x).any() else x)\n",
    "    data[prefix + 'os_mean_fasttext_pa'] = list(map(lambda x, y: np.sqrt(np.sum((x - y) ** 2)),\n",
    "                                                 data['key_text_pre_fasttext'], data['abstract_pre_fasttext']))\n",
    "    data[prefix + 'cos_mean_sif_pa'] = list(map(lambda x, y: np.dot(x, y) / (np.linalg.norm(x) * np.linalg.norm(y)),\n",
    "                                                  data['key_text_pre_sif'], data['abstract_pre_sif']))\n",
    "    data[prefix + 'cos_mean_sif_pa'] = data[prefix + 'cos_mean_sif_pa'].apply(\n",
    "        lambda x: np.nan if np.isnan(x).any() else x)\n",
    "    data[prefix + 'os_mean_sif_pa'] = list(map(lambda x, y: np.sqrt(np.sum((x - y) ** 2)),\n",
    "                                                 data['key_text_pre_sif'], data['abstract_pre_sif']))\n",
    "    data[prefix + 'cos_mean_bert_pa'] = list(map(lambda x, y: np.dot(x, y) / (np.linalg.norm(x) * np.linalg.norm(y)),\n",
    "                                                  data['key_text_bert'], data['abstract_bert']))\n",
    "    data[prefix + 'cos_mean_bert_pa'] = data[prefix + 'cos_mean_bert_pa'].apply(\n",
    "        lambda x: np.nan if np.isnan(x).any() else x)\n",
    "    data[prefix + 'os_mean_bert_pa'] = list(map(lambda x, y: np.sqrt(np.sum((x - y) ** 2)),\n",
    "                                                 data['key_text_bert'], data['abstract_bert']))\n",
    "    data[prefix + 'cos_mean_biobert_pa'] = list(map(lambda x, y: np.dot(x, y) / (np.linalg.norm(x) * np.linalg.norm(y)),\n",
    "                                                  data['key_text_biobert'], data['abstract_biobert']))\n",
    "    data[prefix + 'cos_mean_biobert_pa'] = data[prefix + 'cos_mean_biobert_pa'].apply(\n",
    "        lambda x: np.nan if np.isnan(x).any() else x)\n",
    "    data[prefix + 'os_mean_biobert_pa'] = list(map(lambda x, y: np.sqrt(np.sum((x - y) ** 2)),\n",
    "                                                 data['key_text_biobert'], data['abstract_biobert']))\n",
    "    data[prefix + 'cos_mean_s2v_pa'] = list(map(lambda x, y: np.dot(x, y) / (np.linalg.norm(x) * np.linalg.norm(y)),\n",
    "                                                  data['key_text_s2v'], data['abstract_s2v']))\n",
    "    data[prefix + 'cos_mean_s2v_pa'] = data[prefix + 'cos_mean_s2v_pa'].apply(\n",
    "        lambda x: np.nan if np.isnan(x).any() else x)\n",
    "    data[prefix + 'os_mean_s2v_pa'] = list(map(lambda x, y: np.sqrt(np.sum((x - y) ** 2)),\n",
    "                                                 data['key_text_s2v'], data['abstract_s2v']))\n",
    "\n",
    "    # mhd\n",
    "    data[prefix + 'mhd_mean_word2vec_pa'] = list(map(lambda x, y: np.nan if np.isnan(x).any() or np.isnan(y).any() else\n",
    "    np.linalg.norm(x - y, ord=1), data['key_text_pre_vec'], data['abstract_pre_vec']))\n",
    "    data[prefix + 'mhd_mean_fasttext_pa'] = list(map(lambda x, y: np.nan if np.isnan(x).any() or np.isnan(y).any() else\n",
    "    np.linalg.norm(x - y, ord=1), data['key_text_pre_vec'], data['abstract_pre_fasttext']))\n",
    "    data[prefix + 'mhd_mean_sif_pa'] = list(map(lambda x, y: np.nan if np.isnan(x).any() or np.isnan(y).any() else\n",
    "    np.linalg.norm(x - y, ord=1), data['key_text_pre_sif'], data['abstract_pre_sif']))\n",
    "    data[prefix + 'mhd_mean_bert_pa'] = list(map(lambda x, y: np.nan if np.isnan(x).any() or np.isnan(y).any() else\n",
    "    np.linalg.norm(x - y, ord=1), data['key_text_bert'], data['abstract_bert']))\n",
    "    data[prefix + 'mhd_mean_biobert_pa'] = list(map(lambda x, y: np.nan if np.isnan(x).any() or np.isnan(y).any() else\n",
    "    np.linalg.norm(x - y, ord=1), data['key_text_biobert'], data['abstract_biobert']))\n",
    "    data[prefix + 'mhd_mean_s2v_pa'] = list(map(lambda x, y: np.nan if np.isnan(x).any() or np.isnan(y).any() else\n",
    "    np.linalg.norm(x - y, ord=1), data['key_text_s2v'], data['abstract_s2v']))\n",
    "\n",
    "    #append\n",
    "    data[prefix + 'cos_mean_word2vec_2'] = list(map(lambda x, y: np.dot(x, y) / (np.linalg.norm(x) * np.linalg.norm(y)),\n",
    "                                                  data['description_text_pre_vec'], data['title_pro_vec']))\n",
    "    data[prefix + 'cos_mean_word2vec_2'] = data[prefix + 'cos_mean_word2vec_2'].apply(\n",
    "        lambda x: np.nan if np.isnan(x).any() else x)\n",
    "    data[prefix + 'os_mean_word2vec_2'] = list(map(lambda x, y: np.sqrt(np.sum((x - y) ** 2)),\n",
    "                                                 data['description_text_pre_vec'], data['title_pro_vec']))\n",
    "    data[prefix + 'cos_mean_fasttext_2'] = list(map(lambda x, y: np.dot(x, y) / (np.linalg.norm(x) * np.linalg.norm(y)),\n",
    "                                                  data['description_text_pre_fasttext'], data['title_pro_fasttext']))\n",
    "    data[prefix + 'cos_mean_fasttext_2'] = data[prefix + 'cos_mean_fasttext_2'].apply(\n",
    "        lambda x: np.nan if np.isnan(x).any() else x)\n",
    "    data[prefix + 'os_mean_fasttext_2'] = list(map(lambda x, y: np.sqrt(np.sum((x - y) ** 2)),\n",
    "                                                 data['description_text_pre_fasttext'], data['title_pro_fasttext']))\n",
    "    data[prefix + 'cos_mean_sif_2'] = list(map(lambda x, y: np.dot(x, y) / (np.linalg.norm(x) * np.linalg.norm(y)),\n",
    "                                                  data['description_text_pre_sif'], data['title_pro_sif']))\n",
    "    data[prefix + 'cos_mean_sif_2'] = data[prefix + 'cos_mean_sif_2'].apply(\n",
    "        lambda x: np.nan if np.isnan(x).any() else x)\n",
    "    data[prefix + 'os_mean_sif_2'] = list(map(lambda x, y: np.sqrt(np.sum((x - y) ** 2)),\n",
    "                                                 data['description_text_pre_sif'], data['title_pro_sif']))\n",
    "    data[prefix + 'cos_mean_bert_2'] = list(map(lambda x, y: np.dot(x, y) / (np.linalg.norm(x) * np.linalg.norm(y)),\n",
    "                                                  data['description_text_bert'], data['title_bert']))\n",
    "    data[prefix + 'cos_mean_bert_2'] = data[prefix + 'cos_mean_bert_2'].apply(\n",
    "        lambda x: np.nan if np.isnan(x).any() else x)\n",
    "    data[prefix + 'os_mean_bert_2'] = list(map(lambda x, y: np.sqrt(np.sum((x - y) ** 2)),\n",
    "                                                 data['description_text_bert'], data['title_bert']))\n",
    "    data[prefix + 'cos_mean_biobert_2'] = list(map(lambda x, y: np.dot(x, y) / (np.linalg.norm(x) * np.linalg.norm(y)),\n",
    "                                                  data['description_text_biobert'], data['title_biobert']))\n",
    "    data[prefix + 'cos_mean_biobert_2'] = data[prefix + 'cos_mean_biobert_2'].apply(\n",
    "        lambda x: np.nan if np.isnan(x).any() else x)\n",
    "    data[prefix + 'os_mean_biobert_2'] = list(map(lambda x, y: np.sqrt(np.sum((x - y) ** 2)),\n",
    "                                                 data['description_text_biobert'], data['title_biobert']))\n",
    "    data[prefix + 'cos_mean_s2v_2'] = list(map(lambda x, y: np.dot(x, y) / (np.linalg.norm(x) * np.linalg.norm(y)),\n",
    "                                                  data['description_text_s2v'], data['title_s2v']))\n",
    "    data[prefix + 'cos_mean_s2v_2'] = data[prefix + 'cos_mean_s2v_2'].apply(\n",
    "        lambda x: np.nan if np.isnan(x).any() else x)\n",
    "    data[prefix + 'os_mean_s2v_2'] = list(map(lambda x, y: np.sqrt(np.sum((x - y) ** 2)),\n",
    "                                                 data['description_text_s2v'], data['title_s2v']))\n",
    "    data[prefix + 'cos_mean_bert_pre_2'] = list(map(lambda x, y: np.dot(x, y) / (np.linalg.norm(x) * np.linalg.norm(y)),\n",
    "                                                  data['description_text_bert_pre'], data['title_bert_pre']))\n",
    "    data[prefix + 'cos_mean_bert_pre_2'] = data[prefix + 'cos_mean_bert_pre_2'].apply(\n",
    "        lambda x: np.nan if np.isnan(x).any() else x)\n",
    "    data[prefix + 'os_mean_bert_pre_2'] = list(map(lambda x, y: np.sqrt(np.sum((x - y) ** 2)),\n",
    "                                                 data['description_text_bert_pre'], data['title_bert_pre']))\n",
    "\n",
    "    # mhd\n",
    "    data[prefix + 'mhd_mean_word2vec_2'] = list(map(lambda x, y: np.nan if np.isnan(x).any() or np.isnan(y).any() else\n",
    "    np.linalg.norm(x - y, ord=1), data['description_text_pre_vec'], data['title_pro_vec']))\n",
    "    data[prefix + 'mhd_mean_fasttext_2'] = list(map(lambda x, y: np.nan if np.isnan(x).any() or np.isnan(y).any() else\n",
    "    np.linalg.norm(x - y, ord=1), data['description_text_pre_fasttext'], data['title_pro_fasttext']))\n",
    "    data[prefix + 'mhd_mean_sif_2'] = list(map(lambda x, y: np.nan if np.isnan(x).any() or np.isnan(y).any() else\n",
    "    np.linalg.norm(x - y, ord=1), data['description_text_pre_sif'], data['title_pro_sif']))\n",
    "    data[prefix + 'mhd_mean_bert_2'] = list(map(lambda x, y: np.nan if np.isnan(x).any() or np.isnan(y).any() else\n",
    "    np.linalg.norm(x - y, ord=1), data['description_text_bert'], data['title_bert']))\n",
    "    data[prefix + 'mhd_mean_biobert_2'] = list(map(lambda x, y: np.nan if np.isnan(x).any() or np.isnan(y).any() else\n",
    "    np.linalg.norm(x - y, ord=1), data['description_text_biobert'], data['title_biobert']))\n",
    "    data[prefix + 'mhd_mean_s2v_2'] = list(map(lambda x, y: np.nan if np.isnan(x).any() or np.isnan(y).any() else\n",
    "    np.linalg.norm(x - y, ord=1), data['description_text_s2v'], data['title_s2v']))\n",
    "    data[prefix + 'mhd_mean_bert_pre_2'] = list(map(lambda x, y: np.nan if np.isnan(x).any() or np.isnan(y).any() else\n",
    "    np.linalg.norm(x - y, ord=1), data['description_text_bert_pre'], data['title_bert_pre']))\n",
    "    \n",
    "    # cos\n",
    "    data[prefix + 'cos_mean_word2vec_pa2'] = list(map(lambda x, y: np.dot(x, y) / (np.linalg.norm(x) * np.linalg.norm(y)),\n",
    "                                                  data['description_text_pre_vec'], data['abstract_pre_vec']))\n",
    "    data[prefix + 'cos_mean_word2vec_pa2'] = data[prefix + 'cos_mean_word2vec_pa2'].apply(\n",
    "        lambda x: np.nan if np.isnan(x).any() else x)\n",
    "    data[prefix + 'os_mean_word2vec_pa2'] = list(map(lambda x, y: np.sqrt(np.sum((x - y) ** 2)),\n",
    "                                                 data['description_text_pre_vec'], data['abstract_pre_vec']))\n",
    "    data[prefix + 'cos_mean_fasttext_pa2'] = list(map(lambda x, y: np.dot(x, y) / (np.linalg.norm(x) * np.linalg.norm(y)),\n",
    "                                                  data['description_text_pre_fasttext'], data['abstract_pre_fasttext']))\n",
    "    data[prefix + 'cos_mean_fasttext_pa2'] = data[prefix + 'cos_mean_fasttext_pa2'].apply(\n",
    "        lambda x: np.nan if np.isnan(x).any() else x)\n",
    "    data[prefix + 'os_mean_fasttext_pa2'] = list(map(lambda x, y: np.sqrt(np.sum((x - y) ** 2)),\n",
    "                                                 data['description_text_pre_fasttext'], data['abstract_pre_fasttext']))\n",
    "    data[prefix + 'cos_mean_sif_pa2'] = list(map(lambda x, y: np.dot(x, y) / (np.linalg.norm(x) * np.linalg.norm(y)),\n",
    "                                                  data['description_text_pre_sif'], data['abstract_pre_sif']))\n",
    "    data[prefix + 'cos_mean_sif_pa2'] = data[prefix + 'cos_mean_sif_pa2'].apply(\n",
    "        lambda x: np.nan if np.isnan(x).any() else x)\n",
    "    data[prefix + 'os_mean_sif_pa2'] = list(map(lambda x, y: np.sqrt(np.sum((x - y) ** 2)),\n",
    "                                                 data['description_text_pre_sif'], data['abstract_pre_sif']))\n",
    "    data[prefix + 'cos_mean_bert_pa2'] = list(map(lambda x, y: np.dot(x, y) / (np.linalg.norm(x) * np.linalg.norm(y)),\n",
    "                                                  data['description_text_bert'], data['abstract_bert']))\n",
    "    data[prefix + 'cos_mean_bert_pa2'] = data[prefix + 'cos_mean_bert_pa2'].apply(\n",
    "        lambda x: np.nan if np.isnan(x).any() else x)\n",
    "    data[prefix + 'os_mean_bert_pa2'] = list(map(lambda x, y: np.sqrt(np.sum((x - y) ** 2)),\n",
    "                                                 data['description_text_bert'], data['abstract_bert']))\n",
    "    data[prefix + 'cos_mean_biobert_pa2'] = list(map(lambda x, y: np.dot(x, y) / (np.linalg.norm(x) * np.linalg.norm(y)),\n",
    "                                                  data['description_text_biobert'], data['abstract_biobert']))\n",
    "    data[prefix + 'cos_mean_biobert_pa2'] = data[prefix + 'cos_mean_biobert_pa2'].apply(\n",
    "        lambda x: np.nan if np.isnan(x).any() else x)\n",
    "    data[prefix + 'os_mean_biobert_pa2'] = list(map(lambda x, y: np.sqrt(np.sum((x - y) ** 2)),\n",
    "                                                 data['description_text_biobert'], data['abstract_biobert']))\n",
    "    data[prefix + 'cos_mean_s2v_pa2'] = list(map(lambda x, y: np.dot(x, y) / (np.linalg.norm(x) * np.linalg.norm(y)),\n",
    "                                                  data['description_text_s2v'], data['abstract_s2v']))\n",
    "    data[prefix + 'cos_mean_s2v_pa2'] = data[prefix + 'cos_mean_s2v_pa2'].apply(\n",
    "        lambda x: np.nan if np.isnan(x).any() else x)\n",
    "    data[prefix + 'os_mean_s2v_pa2'] = list(map(lambda x, y: np.sqrt(np.sum((x - y) ** 2)),\n",
    "                                                 data['description_text_s2v'], data['abstract_s2v']))\n",
    "\n",
    "    # mhd\n",
    "    data[prefix + 'mhd_mean_word2vec_pa2'] = list(map(lambda x, y: np.nan if np.isnan(x).any() or np.isnan(y).any() else\n",
    "    np.linalg.norm(x - y, ord=1), data['description_text_pre_vec'], data['abstract_pre_vec']))\n",
    "    data[prefix + 'mhd_mean_fasttext_pa2'] = list(map(lambda x, y: np.nan if np.isnan(x).any() or np.isnan(y).any() else\n",
    "    np.linalg.norm(x - y, ord=1), data['description_text_pre_fasttext'], data['abstract_pre_fasttext']))\n",
    "    data[prefix + 'mhd_mean_sif_pa2'] = list(map(lambda x, y: np.nan if np.isnan(x).any() or np.isnan(y).any() else\n",
    "    np.linalg.norm(x - y, ord=1), data['description_text_pre_sif'], data['abstract_pre_sif']))\n",
    "    data[prefix + 'mhd_mean_bert_pa2'] = list(map(lambda x, y: np.nan if np.isnan(x).any() or np.isnan(y).any() else\n",
    "    np.linalg.norm(x - y, ord=1), data['description_text_bert'], data['abstract_bert']))\n",
    "    data[prefix + 'mhd_mean_biobert_pa2'] = list(map(lambda x, y: np.nan if np.isnan(x).any() or np.isnan(y).any() else\n",
    "    np.linalg.norm(x - y, ord=1), data['description_text_biobert'], data['abstract_biobert']))\n",
    "    data[prefix + 'mhd_mean_s2v_pa2'] = list(map(lambda x, y: np.nan if np.isnan(x).any() or np.isnan(y).any() else\n",
    "    np.linalg.norm(x - y, ord=1), data['description_text_s2v'], data['abstract_s2v']))\n",
    "\n",
    "    data[prefix + 'n_gram_sim'], data[prefix + 'sim_numeber_rate'] = get_df_grams(data,2,['key_text_pre','title_pro'])\n",
    "    data[prefix + 'n_gram_sim_pa'], data[prefix + 'sim_numeber_rate_pa'] = get_df_grams(data,2,['key_text_pre','abstract_pre'])\n",
    "    data[prefix + 'n_gram_sim_tri'], data[prefix + 'sim_numeber_rate_tri'] = get_df_grams(data,3,['key_text_pre','title_pro'])\n",
    "    data[prefix + 'n_gram_sim_pa_tri'], data[prefix + 'sim_numeber_rate_pa_tri'] = get_df_grams(data,3,['key_text_pre','abstract_pre'])\n",
    "\n",
    "    #append\n",
    "    data[prefix + 'n_gram_sim_2'], data[prefix + 'sim_numeber_rate_2'] = get_df_grams(data,2,['description_text_pre','title_pro'])\n",
    "    data[prefix + 'n_gram_sim_pa_2'], data[prefix + 'sim_numeber_rate_pa_2'] = get_df_grams(data,2,['description_text_pre','abstract_pre'])\n",
    "    data[prefix + 'n_gram_sim_tri_2'], data[prefix + 'sim_numeber_rate_tri_2'] = get_df_grams(data,3,['description_text_pre','title_pro'])\n",
    "    data[prefix + 'n_gram_sim_pa_tri_2'], data[prefix + 'sim_numeber_rate_pa_tri_2'] = get_df_grams(data,3,['description_text_pre','abstract_pre'])\n",
    "\n",
    "    data[prefix + 'bm_25_all'] = list(map(lambda x, y: get_bm25(x, y), data['paper_id'], data['key_text_pre']))\n",
    "    #append\n",
    "    data[prefix + 'bm_25_all_2'] = list(map(lambda x, y: get_bm25(x, y), data['paper_id'], data['description_text_pre']))\n",
    "\n",
    "    data[prefix + 'bm25'] = apply_fun(data[['description_id', 'key_text_pre', 'title_pro']])\n",
    "    data[prefix + 'bm25_pa'] = apply_fun(data[['description_id', 'key_text_pre', 'abstract_pre']])\n",
    "\n",
    "    #append\n",
    "    data[prefix + 'bm25_2'] = apply_fun(data[['description_id', 'description_text_pre', 'title_pro']])\n",
    "    data[prefix + 'bm25_pa_2'] = apply_fun(data[['description_id', 'description_text_pre', 'abstract_pre']])\n",
    "\n",
    "    feat = []\n",
    "    for col in data.columns:\n",
    "        if re.match(prefix, col) != None:\n",
    "            feat.append(col)\n",
    "    data = data[feat]\n",
    "\n",
    "    return data\n",
    "\n",
    "word2vec_path = model_path+'word2vec.model'\n",
    "fasttext_path = model_path+'fasttext2.bin'\n",
    "sif_path = model_path+'sif.model'\n",
    "\n",
    "vec_model = Word2Vec.load(word2vec_path)\n",
    "fasttext_model = fasttext.load_model(fasttext_path)\n",
    "sif_model = BaseSentence2VecModel.load(sif_path)\n",
    "\n",
    "with open(model_path+'tag2idx.pkl', 'rb') as f:\n",
    "    tag2idx = pickle.load(f)\n",
    "\n",
    "t1 = time.time()\n",
    "cols = ['description_id', 'description_text', 'description_text_pre',\n",
    "        'key_text_pre', 'paper_id', 'keywords', 'title_pro', 'abstract_pre']\n",
    "\n",
    "if not test_only:\n",
    "    train_data = pd.read_csv(data_path+'train_data_merge_{}.csv'.format(n))[cols]\n",
    "    print(train_data.shape)\n",
    "    train_feat = pool_extract(data=train_data.fillna(''),\n",
    "                              f=get_features,\n",
    "                              vec_model=vec_model,\n",
    "                              postpostfix='_train',\n",
    "                              chunk_size=train_data.shape[0]//workers+1,\n",
    "                              worker=workers)\n",
    "    train_feat.to_csv(data_path+'train_data_merge_{}_featall.csv'.format(n), index=False)\n",
    "    print(train_feat.shape)\n",
    "    del train_data, train_feat\n",
    "    gc.collect()\n",
    "\n",
    "if not train_only:\n",
    "    test_data = pd.read_csv(data_path+'test_data_merge_{}_{}.csv'.format(n, paper_thd))[cols]\n",
    "    print(test_data.shape)\n",
    "    test_feat = pool_extract(data=test_data.fillna(''),\n",
    "                             f=get_features,\n",
    "                             vec_model=vec_model,\n",
    "                             postpostfix='_test',\n",
    "                             chunk_size=test_data.shape[0]//workers+1,\n",
    "                             worker=workers)\n",
    "    test_feat.to_csv(data_path+'test_data_merge_{}_{}_featall.csv'.format(n, paper_thd), index=False)\n",
    "    print(test_feat.shape)\n",
    "\n",
    "print('success')\n",
    "t2 = time.time()\n",
    "print((t2-t1) / 60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
